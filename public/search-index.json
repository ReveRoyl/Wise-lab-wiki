[{"content":"","date":"2023-09-07","id":0,"permalink":"/docs/computational_modelling/behavioural_modelling_package/","summary":"","tags":[],"title":"Behavioural Modelling Package"},{"content":"","date":"2023-09-07","id":1,"permalink":"/docs/computational_modelling/","summary":"","tags":[],"title":"Computational modelling"},{"content":"","date":"2023-09-07","id":2,"permalink":"/docs/computational_modelling/guide/","summary":"","tags":[],"title":"Guide"},{"content":"","date":"2023-09-07","id":3,"permalink":"/docs/computational_modelling/tutorial/","summary":"","tags":[],"title":"Tutorial"},{"content":"This guide will take you through the process of building trial-by-trial learning models. It assumes some knowledge of how these models work, and is more focused on the implementation.\nJAX After a lot of experimentation with different frameworks, I have settled on JAX as my go-to library for building models. JAX enables you to write high-performance numerical code in Python that runs on CPUs, GPUs, and TPUs. It does this by compiling Python functions to XLA (Accelerated Linear Algebra) operations, which can then be executed on the hardware of your choice. This gives us a lot of flexibility with model-fitting, as we can easily switch between different hardware accelerators without changing our code.\nThe other major benefit of JAX is that the code is virtually identical to NumPy, which makes it very easy to pick up if you are already familiar with NumPy. This is in contrast to other libraries, which have their own APIs (or even languages in the case of Stan) that can take some time to learn.\nArchitecture of a model Here I\u0026rsquo;ll break down how we put together a model in JAX.\nThe update rule I generally try to make the modelling code as modular as possible. This means we have a single function that defines the update rule for every trial (i.e., how much the estimated value changes on a trial, given a set of inputs).\nSo, for example, if we have a simple Rescorla-Wagner model, we might define a function like this:\ndef rescorla_wagner_update(value, reward, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule.\u0026#34;\u0026#34;\u0026#34; return value + alpha * (reward - value)\rNote: The code in this guide is simplified for clarity and has a lot of things missing.\nThis function takes the current value estimate, the reward received on the trial, and the learning rate alpha as inputs.\nLooping over trials In ordinary python code, we would loop over trials using a for loops, such as:\n# Initialise a numpy array of values, assuming 100 trials values = np.ones(100) * 0.5 # Generate some trial outcomes outcomes = np.random.binomial(1, 0.5, 100) # Set the learning rate alpha = 0.1 # Loop over trials, up to the final trial (we don\u0026#39;t need to update the value on the final trial) for trial, outcome in enumerate(outcomes[:-1]): values[trial + 1] = rescorla_wagner_update(values[trial], outcome, alpha)\rHowever, this will NOT work when using JAX. Instead, we need to use a function called jax.lax.scan, which allows us to loop over trials in a way that can be compiled by JAX. This ultimately makes the code run much faster, so it\u0026rsquo;s worth the extra effort.\nThe scan function can be a little confusing. Essentially, it is designed to apply a function to a sequence of inputs, and accumulate the results. So, in our case, we want to apply the rescorla_wagner_update function to a sequence of outcomes, and accumulate the values.\nThe documentation on scan is quite good, so I recommend reading it here if you\u0026rsquo;re not sure how it works.\nHere\u0026rsquo;s how we would implement the Rescorla-Wagner model using scan:\n# Initial value estimate initial_value = 0.5 # Generate some trial outcomes outcomes = np.random.binomial(1, 0.5, 100) # Set the learning rate alpha = 0.1 # Use partial to \u0026#34;bake in\u0026#34; the learning rate rescorla_wagner_update_partial = partial(rescorla_wagner_update, alpha=alpha) # Loop over trials _, values = jax.lax.scan( rescorla_wagner_update, # The function we want to apply initial_value # The starting value outcomes # The outcomes on each trial )\r‚ùìWhat does partial do?\nThe partial function is a way of \u0026ldquo;baking in\u0026rdquo; some of the arguments to a function.\nIn this case, we are baking in the alpha argument to the rescorla_wagner_update function, as we can\u0026rsquo;t pass in extra arguments to the function when using scan.\nThis means that when we call rescorla_wagner_update_partial, we only need to pass in the value and reward arguments, and the alpha argument is already set to 0.1.\nThis might seem a bit complicated, but it gives us the same output as the traditional for loop but much faster.\nReturning multiple outputs Sometimes we might want to keep track of other things during the model fitting process, such as the prediction error on each trial. We can do this by modifying the rescorla_wagner_update function to return multiple outputs, and then unpacking them in the scan function.\nHere\u0026rsquo;s an example:\ndef rescorla_wagner_update(value, reward, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule.\u0026#34;\u0026#34;\u0026#34; prediction_error = reward - value new_value = value + alpha * prediction_error return new_value, prediction_error # Loop over trials _, values, prediction_errors = jax.lax.scan( rescorla_wagner_update, # The function we want to apply initial_value, # The starting value outcomes # The outcomes on each trial )\rOther things to watch out for with JAX There are a few other areas where JAX does things a little differently. Helpfully, there\u0026rsquo;s a whole page on this in the JAX documentation, which I recommend reading here.\nIf/else statements One thing to watch out for is that JAX doesn\u0026rsquo;t support if statements in the same way of Python. So, for example in standard Python we might implement an asymmetric learning rate like this:\ndef rescorla_wagner_update(value, reward, alpha_p, alpha_n): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule.\u0026#34;\u0026#34;\u0026#34; prediction_error = reward - value if prediction_error \u0026gt; 0: new_value = value + alpha_p * prediction_error else: new_value = value + alpha_n * prediction_error return new_value, prediction_error\rWe can\u0026rsquo;t do this in JAX. While JAX does have a jax.lax.cond function that allows you to do conditional branching, it\u0026rsquo;s often easier to rewrite the function to avoid the need for if statements. So, in this case, we might rewrite the function like this:\ndef rescorla_wagner_update(value, reward, alpha_p, alpha_n): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule.\u0026#34;\u0026#34;\u0026#34; prediction_error = reward - value new_value = value + (alpha_p * prediction_error * (prediction_error \u0026gt; 0) + alpha_n * prediction_error * (prediction_error \u0026lt;= 0)) return new_value, prediction_error\rHere, we create a binary variable that is 1 if the prediction error is positive and 0 otherwise. We then multiply the learning rate by this binary variable, which has the effect of setting the learning rate to alpha_p if the prediction error is positive and alpha_n otherwise.\nJIT compiling JAX has a jit function that allows you to compile a function for faster execution (JIT stands for \u0026ldquo;just in time\u0026rdquo;). If you don\u0026rsquo;t do this, the function will run in \u0026ldquo;interpreted\u0026rdquo; mode, which is much slower. You can use the jit function like this:\n@jax.jit def rescorla_wagner_update(value, reward, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule.\u0026#34;\u0026#34;\u0026#34; prediction_error = reward - value new_value = value + alpha * prediction_error return new_value, prediction_error\rOr:\nrescorla_wagner_update = jax.jit(rescorla_wagner_update)\rDynamic shapes One of the reasons why JAX-compiled code is fast is because it is inflexible. Ordinarily, I could write a function like this:\n# Define trial outcomes for 3 actions outcomes = np.random.binomial(1, 0.5, (100, 3)) def rescorla_wagner_iterator(value, outcomes, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule over a series of trials.\u0026#34;\u0026#34;\u0026#34; # Get the number of actions n_actions = outcomes.shape[1] # Initialise value estimates for each action values = np.ones(n_actions) * 0.5 # Loop over trials, up to the final trial (we don\u0026#39;t need to update the value on the final trial) for trial in range(outcomes.shape[0] - 1): values[trial + 1] = rescorla_wagner_update(values[trial], outcomes[trial, :], alpha) # Set the learning rate alpha = 0.1 # Run the iterator values = rescorla_wagner_iterator(0.5, outcomes, alpha)\rThis function will determine the shape of its values array based on the number of actions present in the outcomes array. So if I changed the outcomes array to have 4 actions, the values array would automatically adjust to have 4 elements.\nThis isn\u0026rsquo;t possible with JAX as it requires static shapes. This means that the shape of the arrays must be known at compile time. This can be a bit of a pain, but it\u0026rsquo;s a trade-off for the speed that JAX provides. Flexibility slows our code down.\nOne way around this is to use the static_argnums argument in the jit function, which allows you to specify which arguments have static shapes. This means that JAX will compile the function with the shapes that it is given. If you try to call the function with a different shape, you\u0026rsquo;ll get an error.\n# Define trial outcomes for 3 actions outcomes = np.random.binomial(1, 0.5, (100, 3)) def rescorla_wagner_iterator(value, outcomes, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule over a series of trials.\u0026#34;\u0026#34;\u0026#34; # Get the number of actions n_actions = outcomes.shape[1] # Initialise value estimates for each action values = np.ones(n_actions) * 0.5 # Loop over trials, up to the final trial (we don\u0026#39;t need to update the value on the final trial) for trial in range(outcomes.shape[0] - 1): values[trial + 1] = rescorla_wagner_update(values[trial], outcomes[trial, :], alpha) # JIT compile rescorla_wagner_iterator = jax.jit(rescorla_wagner_iterator, static_argnums=(1,)) # Set the learning rate alpha = 0.1 # Run the iterator values = rescorla_wagner_iterator(0.5, outcomes, alpha)\rThis will compile the function so that it expects the outcomes array to have 3 actions. If you try to call the function with an array that has a different number of actions, you\u0026rsquo;ll get an error. Otherwise, it will work as expected.\n","date":"2023-09-07","id":4,"permalink":"/docs/computational_modelling/guide/1.-building-basic-models/","summary":"This guide will take you through the process of building trial-by-trial learning models. It assumes some knowledge of how these models work, and is more focused on the implementation.","tags":[],"title":"1. Building basic models"},{"content":"In the previous section, we discussed how to model a single participant. In this section, we will discuss how to model multiple participants.\nApplying our model across multiple participants Nested loops When we model multiple participants, we can use the same model code for each participant. In standard Python, we might do this using nested for loops.\nFor example:\n# Number of participants n_participants = 10 # Initialise a numpy array of values, assuming 100 trials values = np.ones((n_participants, 100)) * 0.5 # Generate some trial outcomes - these are the same for every participant outcomes = np.random.binomial(1, 0.5, 100) # Set learning rates for each participant alphas = np.random.uniform(0.1, 0.5, n_participants) # Loop over participants for participant in range(n_participants): # Loop over trials for trial, outcome in enumerate(outcomes): values[participant, trial + 1] = rescorla_wagner_update(values[participant, trial], outcome, alphas[participant])\rVectorisation This code is straightforward, but it can be slow for large numbers of participants or trials. We can speed this up through vectorisation, which is a technique that allows us to perform operations on entire arrays at once.\nFor example:\n# Number of participants n_participants = 10 # Initialise a numpy array of values, assuming 100 trials values = np.ones((n_participants, 100)) * 0.5 # Generate some trial outcomes - these are the same for every participant outcomes = np.random.binomial(1, 0.5, 100) # Set learning rates for each participant alphas = np.random.uniform(0.1, 0.5, n_participants) # Loop over trials for trial, outcome in enumerate(outcomes): values[:, trial + 1] = rescorla_wagner_update(values[:, trial], outcome, alphas)\rThis code is much faster than the nested loop version, as it takes advantage of the underlying NumPy operations to perform the update across all participants in a single step.\nVectorisation in JAX Vectorisation is fairly straightforward in the above example, but can be more complicated in many situations. JAX provides a straightforward way to vectorise code using the vmap function. The vmap function allows us to apply a function to a batch of inputs, and returns a batch of outputs. Because it compiles the code using XLA, it can be much faster than using standard Python loops or NumPy operations.\nTo facilitate this, we can create a function that runs our update rule function for a single participant over a bunch of trials, which we can later vectorise across participants.\nFor example:\ndef rescorla_wagner_trial_iterator(value, outcomes, alpha): \u0026#34;\u0026#34;\u0026#34;Update the value estimate using the Rescorla-Wagner rule over a series of trials.\u0026#34;\u0026#34;\u0026#34; # Use partial to \u0026#34;bake in\u0026#34; the learning rate rescorla_wagner_update_partial = partial(rescorla_wagner_update, alpha=alpha) # Loop over trials _, values = jax.lax.scan( rescorla_wagner_update, # The function we want to apply initial_value # The starting value outcomes # The outcomes on each trial ) return values\rWe can then use the vmap function to create a vectorised version of this function that applies the update rule to each participant.\n# Number of participants n_participants = 10 # Generate some trial outcomes - these are the same for every participant outcomes = np.random.binomial(1, 0.5, 100) # Set learning rates for each participant alphas = np.random.uniform(0.1, 0.5, n_participants) # Vectorise the function across participants rescorla_wagner_trial_iterator_vmap = jax.vmap(rescorla_wagner_trial_iterator, in_axes=(None, None, 0)) # Use this function values = rescorla_wagner_trial_iterator_vmap(0.5, outcomes, alphas)\r","date":"2023-09-07","id":5,"permalink":"/docs/computational_modelling/guide/2.-modelling-multiple-participants/","summary":"In the previous section, we discussed how to model a single participant. In this section, we will discuss how to model multiple participants.","tags":[],"title":"2. Modelling multiple participants"},{"content":"The previous sections outline how to generate simulated data using a computational model. In this section, we will discuss how to use Markov Chain Monte Carlo (MCMC) sampling to fit a model to data.\nWhat is MCMC sampling? MCMC sampling is a method for estimating the parameters of a model by sampling from the posterior distribution of the parameters. This means we can estimate a full probability distribution for each parameter, rather than just a point estimate. As we\u0026rsquo;re thinking about probability distributions, these models are often referred to as \u0026ldquo;Bayesian\u0026rdquo; or \u0026ldquo;probabilistic\u0026rdquo; models.\nMCMC sampling is the \u0026ldquo;gold standard\u0026rdquo; for fitting computational models as it is accurate and provides a full representation of the uncertainty in the parameter estimates. However, it can be slow and complicated (the fact that a whole paper has been written on debugging these models is a testament to this).\nMCMC sampling using NumPyro In this section, we will use the NumPyro library to perform MCMC sampling. NumPyro is a probabilistic programming library that is built on top of JAX. Because it is built on JAX, NumPyro is able to take advantage of JAX\u0026rsquo;s ability to compile code to run fast on different hardware (e.g., GPUs). As a result, it is able to perform MCMC sampling quickly and efficiently.\nCreating a NumPyro model Terminology can get confusing within the world of computational model fitting, as we tend to use the word \u0026ldquo;model\u0026rdquo; to refer to different things. Generally, we\u0026rsquo;ll talk about two different types of models:\nBehavioural model: This is the model that generates the behavioural data. This is the model we\u0026rsquo;ve been discussing in the previous sections. Statistical model: This is the model that describes the relationship between the parameters of the behavioural model and the data. This is the model we\u0026rsquo;re going to discuss in this section. In NumPyro, we define the statistical model using a Python function. So, for our Rescorla-Wagner model, we might define a statistical model like this:\ndef model(outcomes, observations, n_participants): # Define the priors for the learning rate alpha = numpyro.sample(\u0026#34;alpha\u0026#34;, dist.Uniform(0, 1), sample_shape=(n_participants,)) # Define the priors for the softmax temperature parameter beta = numpyro.sample(\u0026#34;beta\u0026#34;, dist.Uniform(0, 1), sample_shape=(n_participants,)) # Get values from our model values = rescorla_wagner_trial_iterator_vmap(0.5, outcomes, alpha) # Apply a softmax function to the values choice_probabilities = softmax(values, beta) # Define the likelihood numpyro.sample(\u0026#34;obs\u0026#34;, dist.Binomial(1, choice_probabilities, obs=observations))\rTo break this down:\nPriors # Define the priors for the learning rate alpha = numpyro.sample(\u0026#34;alpha\u0026#34;, dist.Uniform(0, 1), sample_shape=(n_participants,)) # Define the priors for the softmax temperature parameter beta = numpyro.sample(\u0026#34;beta\u0026#34;, dist.Uniform(0, 1), sample_shape=(n_participants,))\rNote: We\u0026rsquo;ve included a softmax function here to transalte the values into choice probabilities. This is a common step in models that generate choices.\nWe define the priors for the learning rate and softmax temperature parameter. These are the parameters we want to estimate. Here we\u0026rsquo;ve used a uniform distribution, but you can use any appropriate distribution. Given that these values must lie between 0 and 1, you could also use a Beta distribution.\nFor example:\nalpha = numpyro.sample(\u0026#34;alpha\u0026#34;, dist.Beta(2, 2), sample_shape=(n_participants,))\rThe sample_shape argument allows us to sample multiple values for each participant. This is useful when we want to estimate the parameters for multiple participants.\nBehavioural model Next, we generate data from our behavioural model using the parameters we\u0026rsquo;ve sampled:\n# Get values from our model values = rescorla_wagner_trial_iterator_vmap(0.5, outcomes, alpha)\rThis does exactly what we\u0026rsquo;ve described in the previous sections: it generates values from the Rescorla-Wagner model for each participant.\nWe also use a softmax function to convert these values into choice probabilities:\n# Apply a softmax function to the values choice_probabilities = softmax(values, beta)\rLikelihood Finally, we define the likelihood of the data given the model:\nnumpyro.sample(\u0026#34;obs\u0026#34;, dist.Binomial(1, choice_probabilities, obs=observations))\rThis is a binomial likelihood, as we\u0026rsquo;re assuming that the data are binary (i.e., the participant either chose option A or option B). The obs argument specifies the observed data, which in this case is the choices made by the participant.\nThis is ultimately the metric used to fit the model to the data. The MCMC algorithm will sample values for alpha and beta that maximise the likelihood of the data given the model.\nRunning MCMC sampling Once we\u0026rsquo;ve defined our model, we can run MCMC sampling using NumPyro. This is done using the numpyro.infer.MCMC class:\n# Set up the MCMC sampler mcmc = MCMC(NUTS(model), num_samples=4000, num_warmup=1000, num_chains=4) # Get a random key rng_key = jax.random.PRNGKey(seed) # Get number of participants n_participants = observations.shape[0] # Run the MCMC sampling mcmc.run(rng_key, outcomes, observations, n_participants)\rNote: We assume here that observations is a NumPy array containing the observed data and outcomes is a NumPy array containing the task outcomes.\nThis will run the MCMC sampling algorithm and store the samples in the mcmc object. We can then use the samples to estimate the posterior distribution of the parameters.\nFor example, we can use the mcmc.get_samples() method to get the samples from the MCMC run:\nsamples = mcmc.get_samples()\rSettings for MCMC sampling In general, the default settings for MCMC sampling in NumPyro are pretty good. However, there are some things you may want to adjust.\nnum_samples: The number of samples to draw from the posterior distribution. More samples will give you a more accurate estimate of the posterior distribution, but will take longer to run. In general, something like 4000 samples is a good starting point, but more is better if feasible. num_warmup: The number of warmup samples to draw before starting to draw samples from the posterior distribution. Warmup samples are used to \u0026ldquo;tune\u0026rdquo; the sampler and are not included in the final samples. In general, something like 1000 warmup samples is a good starting point. num_chains: The number of chains to run. More chains will give you a more accurate estimate of the posterior distribution, but will take longer to run. I would normally use 4 chains. ","date":"2023-09-07","id":6,"permalink":"/docs/computational_modelling/guide/3.-mcmc-sampling/","summary":"The previous sections outline how to generate simulated data using a computational model. In this section, we will discuss how to use Markov Chain Monte Carlo (MCMC) sampling to fit a model to data.","tags":[],"title":"3. MCMC sampling"},{"content":"The statistical model described in the previous section assumes that participants are independent of one another. In reality, participants are drawn from a group that will often have shared characteristics. Hierarchical models are a way to account for this by allowing parameters to vary between participants and groups of participants. This can be particularly useful when the number of observations per participant is small, as it allows information to be shared between participants, boosting the precision of the estimates.\nA lot has been written about hierarchical Bayesian modelling (e.g. here). Here, we will focus on how to implement hierarchical models in the context of cognitive modelling.\nImplementing a hierarchical model in NumPyro In NumPyro, we can implement a hierarchical model by defining a model within a model. This is similar to how we defined a model within a function in the previous section. The key difference is that we can now define a model within a model, allowing us to specify how parameters vary between participants.\nIn general, it is best to use what is referred to as a \u0026ldquo;non-centred\u0026rdquo; parameterisation. This means that we assume that we have an average parameter value across our sample:\n\\[\r\\mu_{group} = \\text{Normal}(\\mu, \\sigma)\r\\] This says that the group mean is normally distributed with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\).\nWe can define this in NumPyro as:\nmu_group = numpyro.sample(\u0026#34;mu_group\u0026#34;, dist.Normal(0, 1))\rHere, we set the prior for the group mean to be a normal distribution with a mean of 0 and a standard deviation of 1. This will depend on the specific model you are fitting and the ranges of its parameters.\nWe also assume that the participants are distributed around this group mean, and on average their distribution follows a standard deviation parameter \\(\\sigma_{group}\\):\n\\[\r\\sigma_{group} = \\text{HalfNormal}(\\sigma)\r\\] This says that the group standard deviation is half-normal distributed with a standard deviation of \\(\\sigma\\).\nWe can define this in NumPyro as:\nsigma_group = numpyro.sample(\u0026#34;sigma_group\u0026#34;, dist.HalfNormal(1))\rFinally, we define the individual participants\u0026rsquo; parameter values. We first assume that each participant\u0026rsquo;s parameter value is offset from the group mean by some amount:\n\\[\roffset_{participant} = \\text{Normal}(mu, sigma)\r\\] This says that the offset for each participant is normally distributed with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\).\nWe can define this in NumPyro as:\noffset_participant = numpyro.sample(\u0026#34;offset_participant\u0026#34;, dist.Normal(mu_group, sigma_group), sample_shape=(n_participants,)\rFinally, we combine this offset parameter with the group mean and standard deviation. Each participant\u0026rsquo;s parameter value is assumed to be offset from the group mean by a multiple of the standard deviation.\n\\[\r\\theta_{participant} = \\mu_{group} + offset_{participant} \\times \\sigma_{group}\r\\] We can define this in NumPyro as:\ntheta_participant = mu_group + offset_participant * sigma_group\rSo, together, for a given parameter we might have:\ngroup_mean = numpyro.sample(\u0026#34;group_mean\u0026#34;, dist.Normal(0, 1)) group_std = numpyro.sample(\u0026#34;group_std\u0026#34;, dist.HalfNormal(1)) offset = numpyro.sample(\u0026#34;offset\u0026#34;, dist.Normal(group_mean, group_std), sample_shape=(n_participants,)) theta = group_mean + offset * group_std\rBoth group_mean and group_std are shared across all participants, while offset is specific to each participant. As a result, group_mean and group_std are scalars (i.e., they are a single value), while offset and theta are vectors (i.e., they have a value for each participant).\nMaking things more efficient This can end up producing a lot of code, especially if you have many parameters. To make things more efficient, you can define a function that generates the hierarchical model for a single parameter, and then use this function to generate the model for each parameter.\nFor example:\ndef create_subject_params( name: str, n_subs: int ) -\u0026gt; Union[dist.Normal, dist.HalfNormal, dist.Normal]: \u0026#34;\u0026#34;\u0026#34; Creates group mean, group sd and subject-level offset parameters. Args: name (str): Name of the parameter n_subs (int): Number of subjects Returns: Union[dist.Normal, dist.HalfNormal, dist.Normal]: Group mean, group sd, and subject-level offset parameters \u0026#34;\u0026#34;\u0026#34; group_mean = numpyro.sample(\u0026#34;{0}_group_mean\u0026#34;.format(name), dist.Normal(0, 1)) group_sd = numpyro.sample(\u0026#34;{0}_group_sd\u0026#34;.format(name), dist.HalfNormal(1)) offset = numpyro.sample( \u0026#34;{0}_offset\u0026#34;.format(name), dist.Normal(0, 1), sample_shape=(n_subs,) ) return group_mean, group_sd, offset\r","date":"2023-09-07","id":7,"permalink":"/docs/computational_modelling/guide/4.-hierarchical-models/","summary":"The statistical model described in the previous section assumes that participants are independent of one another. In reality, participants are drawn from a group that will often have shared characteristics.","tags":[],"title":"4. Hierarchical models"},{"content":"The best way to visualise and check the results when using sampling is to use the ArviZ library. This library is designed to work with a range of sampling packages, including NumPyro. It provides a range of functions for visualising the results of sampling, including trace plots, density plots, and posterior predictive checks.\nI won\u0026rsquo;t go into detail about how to use ArviZ here, as the ArviZ documentation is very good and covers everything you could need.\n","date":"2023-09-07","id":8,"permalink":"/docs/computational_modelling/guide/5.-visualising-mcmc-results/","summary":"The best way to visualise and check the results when using sampling is to use the ArviZ library. This library is designed to work with a range of sampling packages, including NumPyro.","tags":[],"title":"5. Visualising MCMC results"},{"content":"Coming soon\u0026hellip;\n","date":"2023-09-07","id":9,"permalink":"/docs/computational_modelling/guide/6.-simulation-based-inference/","summary":"Coming soon\u0026hellip;","tags":[],"title":"6. Simulation based inference"},{"content":"","date":"2023-09-07","id":10,"permalink":"/docs/online_experiments/","summary":"","tags":[],"title":"Online experiments"},{"content":"","date":"2023-09-07","id":11,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"","date":"2023-09-07","id":12,"permalink":"/","summary":"","tags":[],"title":"Wise Lab Wiki"},{"content":"In order to standardise the way we build models and make our modelling code maximally reusable, we have developed a package that contains a set of functions that can be used to build behavioural models. This package is called behavioural_modelling and is available on our GitHub repository here.\nThe package is built on top of JAX, ensuring that it is fast and can be run on different hardware (e.g., GPUs). It is primarily focused on trialwise learning models.\n","date":"0001-01-01","id":13,"permalink":"/docs/computational_modelling/behavioural_modelling_package/1.-overview/","summary":"In order to standardise the way we build models and make our modelling code maximally reusable, we have developed a package that contains a set of functions that can be used to build behavioural models.","tags":[],"title":"1. Overview"},{"content":"This tutorial will take you through the implementation of a commonly-used reinforcement learning model, the Rescorla-Wagner model. We will implement an asymmetric version of this model that allows for differential updating of value estimates in response to positive and negative prediction errors (see here).\nWe will implement this using JAX. In all honesty, JAX is not the most user-friendly library for beginners, but it is incredibly powerful and efficient. This means that there are various quirks and tricks that need to be used, but once you get the hang of it the process becomes quite easy.\n","date":"0001-01-01","id":14,"permalink":"/docs/computational_modelling/tutorial/1.-overview/","summary":"This tutorial will take you through the implementation of a commonly-used reinforcement learning model, the Rescorla-Wagner model. We will implement an asymmetric version of this model that allows for differential updating of value estimates in response to positive and negative prediction errors (see here).","tags":[],"title":"1. Overview"},{"content":"Implementing an update function First, we need to create a function that implements the core update method for our model (i.e., how it updates its value estimate in response to the outcomes it has received).\nImports First, we import necessary packages.\nimport jax import numpy as np import jax.numpy as jnp from typing import Tuple\rThe Rescorla-Wagner update rule In the standard Rescorla-Wagner model, this occurs as follows:\n$$V_{t+1} = V_t + \\alpha \\times \\delta_t$$ where $V_{t+1}$ is the value estimate at time $t+1$, $V_t$ is the value estimate at time $t$, $\\alpha$ is the learning rate, and $\\delta_t$ is the prediction error at time $t$.\nThe prediction error $$\\delta_t$$ is calculated as the difference between the reward received at time $t$ and the value estimate at time $t$:\n$$\\delta_t = R_t - V_t$$ where $R_t$ is the reward received at time $t$.\nIntroducing asymmetry We can introduce asymmetry into the model by allowing the learning rate to be different for positive and negative prediction errors. This can be implemented as follows. Our prediction error is calculated as normal:\n$$\\delta_t = R_t - V_t$$ But we now choose our learning rate for the current trial $\\alpha_t$ based on the sign of the prediction error:\n$$\\alpha_t = \\alpha^+ \\text{ if } \\delta_t \u003e 0 \\text{ else } \\alpha^-$$ where $\\alpha^+$ is the learning rate for positive prediction errors, and $\\alpha^-$ is the learning rate for negative prediction errors. The update rule then becomes:\n$$V_{t+1} = V_t + \\alpha_t \\times \\delta_t$$ The update function We can implement this in JAX as follows:\nNote: We will use the @jit decorator to compile the function for faster execution.\n@jax.jit def asymmetric_rescorla_wagner_update( value: float, outcome: float, alpha_p: float, alpha_n: float ) -\u0026gt; Tuple[float, float]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Args: value (float): The current estimated value of a state or action. outcome (float): The actual reward received. alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Calculate the prediction error prediction_error = outcome - value # Set the learning rate based on the sign of the prediction error # Remember - we can\u0026#39;t use if else statements here because JAX doesn\u0026#39;t tolerate them alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error\rA few things to note about this implementation:\nNo if/else statements: As mentioned in this guide, we can\u0026rsquo;t use if/else statements in JAX. Instead, we create binary variables that we can use to determine the learning rate through multiplication. Return values: We return both the value estimate and the prediction error. The value estimate is critical for our model as this is the key quantity we\u0026rsquo;re estimating. The prediction error isn\u0026rsquo;t vital, but can be useful to return (e.g., we might want to plot it later or link it to neural activity). Docstring: We use a docstring to describe the function. This is good practice as it helps others understand what the function does. I like to use Google format for docstrings. Type hints: We use type hints to specify the types of the inputs and outputs. This is good practice as it helps others understand what the function expects and returns. Checking that the functon works If all is well with our implementation, we should be able to run the function and get some output. Let\u0026rsquo;s test this now.\nWe\u0026rsquo;ll set $\\alpha_p$ to a low value and $\\alpha_n$ to a high value, so we can see how the value estimate changes in response to positive and negative prediction errors.\n# Initialize the value, outcome, and learning rates value = 0.5 outcome = 1.0 alpha_p = 0.1 alpha_n = 0.9 # Call the function updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, alpha_p, alpha_n ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Prediction Error: {prediction_error}\u0026#34;)\rUpdated Value: 0.55 Prediction Error: 0.5 Here, we have a positive prediction error and we can see that the value estimate increases by a small amount, as the learning rate for positive prediction errors is low.\nLet\u0026rsquo;s see what happens if we have a negative prediction error.\n# Initialize the value, outcome, and learning rates value = 0.5 outcome = 0 alpha_p = 0.1 alpha_n = 0.9 # Call the function updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, alpha_p, alpha_n ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Prediction Error: {prediction_error}\u0026#34;)\rUpdated Value: 0.04999999999999999 Prediction Error: -0.5 We can see that our estimated value has gone from 0.55 to 0.05 (there\u0026rsquo;s a bit of a precision issue here, but it\u0026rsquo;s close enough). This is because the learning rate for negative prediction errors is high, so the value estimate has decreased by a large amount.\nWorking with arrays Our function can also be applied to arrays. This is useful if we have multiple stimuli/actions that people are learning the value of. We can pass in an array of value estimates and rewards, and get back an array of updated value estimates and prediction errors.\n# Initialize the value, outcome, and learning rates value = np.ones(5) * 0.5 outcome = np.array([1.0, 0.0, 1.0, 0.0, 1.0]) alpha_p = 0.1 alpha_n = 0.9 # Call the function updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, alpha_p, alpha_n ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Prediction Error: {prediction_error}\u0026#34;)\rUpdated Value: [0.55 0.05 0.55 0.05 0.55] Prediction Error: [ 0.5 -0.5 0.5 -0.5 0.5] Making our function more flexible It\u0026rsquo;s quite common in learning tasks that people have to estimate the value of multiple stimuli or actions, but only receive feedback for their chosen option on any given trial. Currently, our function will update the value estimate for all stimuli/actions on every trial, which isn\u0026rsquo;t necessarily what we want to do.\nWe can make our function more flexible by allowing it to update only the value estimate for the chosen option. We can do this by passing in an additional argument that specifies which option was chosen on the current trial. We\u0026rsquo;ll also update the type hints so that they make it clear we can pass in arrays of value estimates and rewards.\nNote: We use the jax.typing.ArrayLike type hint to specify that the input is an array-like object (e.g., a list, tuple, or JAX array). This is useful as it makes it clear that the user can pass in different types of array-like objects (e.g., lists or JAX arrays).\n@jax.jit def asymmetric_rescorla_wagner_update( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, chosen: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0. Args: value (float): The current estimated value of a state or action. outcome (float): The actual reward received. chosen (float): Binary indicator of whether the action was chosen (1) or not (0). alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Calculate the prediction error prediction_error = outcome - value # Set prediction error to 0 for unchosen actions prediction_error = prediction_error * chosen # Set the learning rate based on the sign of the prediction error # Remember - we can\u0026#39;t use if else statements here because JAX doesn\u0026#39;t tolerate them alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error\rHere, we\u0026rsquo;ve incorporated information about which option was chosen by multiplying the prediction error by a binary variable that is 1 if the option was chosen and 0 otherwise. This means that the value estimate for the chosen option will be updated, while the value estimates for the other options will remain the same.\n# Initialize the value, outcome, choices, and learning rates value = np.ones(5) * 0.5 outcome = np.array([1.0, 0.0, 1.0, 0.0, 1.0]) chosen = np.array([1, 1, 0, 0, 0]) alpha_p = 0.1 alpha_n = 0.9 # Call the function updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, chosen, alpha_p, alpha_n ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Prediction Error: {prediction_error}\u0026#34;)\rAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu. Updated Value: [0.55 0.05000001 0.5 0.5 0.5 ] Prediction Error: [ 0.5 -0.5 0. -0. 0. ] We can see that only the values for the chosen options have been updated, while the rest remain at 0.5.\n","date":"0001-01-01","id":15,"permalink":"/docs/computational_modelling/tutorial/2.-implementing-an-update-function/","summary":"Implementing an update function First, we need to create a function that implements the core update method for our model (i.","tags":[],"title":"2. Implementing an update function"},{"content":"Selecting actions In most learning tasks we are asking participants to select different actions or stimuli based on their estimated value. This means that our model not only needs to estimate the value of different options, but also make choices.\nImports First, we import necessary packages.\nimport jax import numpy as np import jax.numpy as jnp from typing import Tuple\rThe Softmax function The softmax function is a common way to convert values into choice probabilities. It is defined as:\n$$ P(a) = \\frac{e^{Q(a) / \\tau}}{\\sum_{a'} e^{Q(a') / \\tau}} $$ where $Q(a)$ is the value of action $a$, and $\\tau$ is a parameter that controls the randomness of the choices (referred to as a temperature parameter). When $\\tau$ is high, the softmax function will output similar probabilities for all actions, while when $\\tau$ is low, the softmax function will output probabilities that are close to 0 or 1.\nEssentially, the softmax function calculates the probability of a given action based on its value relative to the values of all other actions. The higher the value of an action, the higher its probability of being selected.\nFor the sake of simplicity and reproducibility, we\u0026rsquo;ll use an existing implementation of the softmax function from the behavioural_modelling package.\nfrom behavioural_modelling.decision_rules import softmax ?softmax\r\u001b[0;31mSignature:\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u0026gt;\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mCall signature:\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mType:\u001b[0m PjitFunction \u001b[0;31mString form:\u001b[0m \u0026lt;PjitFunction of \u0026lt;function softmax at 0x7f16777c3be0\u0026gt;\u0026gt; \u001b[0;31mFile:\u001b[0m ~/miniconda3/envs/transition_uncertainty/lib/python3.10/site-packages/behavioural_modelling/decision_rules.py \u001b[0;31mDocstring:\u001b[0m Softmax function, with optional temperature parameter. Args: value (ArrayLike): Array of values to apply softmax to, of shape (n_trials, n_bandits) temperature (float, optional): Softmax temperature, in range 0 \u0026gt; inf. Defaults to 1. Returns: ArrayLike: Choice probabilities, of shape (n_trials, n_bandits) Demonstrating the softmax function To demonstrate, we can provide a set of action values and calculate the probabilities of selecting each action according to different temperature parameter values.\nNOTE: The function expects our values to be 2-dimensional, as we\u0026rsquo;ll often want to apply it to a a set of values for multiple stimuli across multiple trials.\n# Initialize the values values = jnp.array([[2.0, 3.0, 1.0]]) # Example temperature parameter values temperature = [0.1, 0.5, 0.9] # Compute the softmax probabilities using each temperature parameter for t in temperature: print(f\u0026#34;Temperature: {t}\u0026#34;) print(np.round(softmax(values, t), 2))\rTemperature: 0.1 [[0. 1. 0.]] Temperature: 0.5 [[0.12 0.87 0.02]] Temperature: 0.9 [[0.22999999 0.7 0.08 ]] Choosing an action We also want to actually choose an action based on these estimated probabilities. Again, we\u0026rsquo;ll use an existing function for this.\nfrom behavioural_modelling.utils import choice_from_action_p ?choice_from_action_p\r\u001b[0;31mSignature:\u001b[0m \u001b[0mchoice_from_action_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\u0026lt;\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mPRNGKey\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f160e170700\u001b[0m\u001b[0;34m\u0026gt;\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m \u001b[0mlapse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m \u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u0026gt;\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mCall signature:\u001b[0m \u001b[0mchoice_from_action_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m \u001b[0;31mType:\u001b[0m PjitFunction \u001b[0;31mString form:\u001b[0m \u0026lt;PjitFunction of \u0026lt;function choice_from_action_p at 0x7f15fc730310\u0026gt;\u0026gt; \u001b[0;31mFile:\u001b[0m ~/miniconda3/envs/transition_uncertainty/lib/python3.10/site-packages/behavioural_modelling/utils.py \u001b[0;31mDocstring:\u001b[0m Choose an action from a set of action probabilities. Can take probabilities in the form of an n-dimensional array, where the last dimension is the number of actions. Noise is added to the choice, with probability `lapse`. This means that on \u0026quot;lapse\u0026quot; trials, the subject will choose an action uniformly at random. Args: key (int): Jax random key probs (np.ndarray): N-dimension array of action probabilities, of shape (..., n_actions) lapse (float, optional): Probability of lapse. Defaults to 0.0. Returns: int: Chosen action Incorporating randomness We want to make sure our choices are not deterministic: if we have an action probability of 0.75 this means we\u0026rsquo;ll only want to choose this action 75% of the time. JAX is a little complex when it comes to randomness, and you need to supply a random \u0026ldquo;key\u0026rdquo; every time you want to generate random numbers. This means that when using this function for choosing actions, you\u0026rsquo;ll need to pass in a key as well.\nBecause we\u0026rsquo;re supplying a random key, the function will always return the same action when is given the same key. This is useful for reproducibility, but it also means that you\u0026rsquo;ll need to pass in a new key every time you want to generate a new action.\n# Get a random key key = jax.random.PRNGKey(0) # Choose an action using the softmax probabilities choice_from_action_p(key, softmax(values, t))\rArray([1], dtype=int32) Incorporating the softmax function into our model As it stands, we\u0026rsquo;ve implemented a model that can estimate the value of different actions. However, we haven\u0026rsquo;t yet implemented a way to select actions based on these values. We can do this by incorporating the softmax function into our model.\nIn order to keep our code as modular as possible, we will create a new function (asymmetric_rescorla_wagner_update_choice) that will use our existing update function to estimate the value of different actions, and then use the softmax function to select an action based on these values, rather than integrating this functionality directly into our existing update function.\n# THIS IS OUR EXISTING UPDATE FUNCTION @jax.jit def asymmetric_rescorla_wagner_update( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, chosen: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0. Args: value (float): The current estimated value of a state or action. outcome (float): The actual reward received. chosen (float): Binary indicator of whether the action was chosen (1) or not (0). alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Calculate the prediction error prediction_error = outcome - value # Set prediction error to 0 for unchosen actions prediction_error = prediction_error * chosen # Set the learning rate based on the sign of the prediction error alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error # THIS IS OUR NEW CHOICE FUNCTION @jax.jit def asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, temperature: float, n_actions: int, key: jax.random.PRNGKey, ) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function. Args: value (jax.typing.ArrayLike): The current value estimate. outcome (jax.typing.ArrayLike): The outcome of the action. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for softmax function. n_actions (int): The number of actions to choose from. key (jax.random.PRNGKey): The random key for the choice function. Returns: Tuple[np.ndarray, Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]]: - updated_value (jnp.ndarray): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]): - value (jax.typing.ArrayLike): The original value estimate. - choice_p (jnp.ndarray): The choice probabilities. - choice (int): The chosen action. - choice_array (jnp.ndarray): The chosen action in one-hot format. \u0026#34;\u0026#34;\u0026#34; # Get choice probabilities choice_p = softmax(value[None, :], temperature).squeeze() # Get choice choice = choice_from_action_p(key, choice_p) # Convert it to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1) # Get the outcome and update the value estimate updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, choice_array, outcome, alpha_p, alpha_n, ) return updated_value, (value, choice_p, choice_array, prediction_error)\rThere is quite a lot going on here, so let\u0026rsquo;s break it down.\n1. Inputs to the function def asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, chosen: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, temperature: float, n_actions: int, key: jax.random.PRNGKey, ) -\u0026gt; np.ndarray:\rAs with our previous function, we provide the current value and the outcome received. Note that we don\u0026rsquo;t need to provide the chosen option as we\u0026rsquo;re generating this from scratch. We also provide the learning rates for positive and negative prediction errors and the temperature parameter for the softmax function.\nWe also need to provide the number of possible actions. This is because we need to generate a one-hot array of the chosen action, and we need to know how long this array should be. Why can we not just infer this from the length of the value array using e.g., value.shape? This is because JAX needs to know the size of the array at compile time, and the size of the value array is not known until runtime. Otherwise, we will get an error when we try to compile the function.\nFinally, we need to provide a random key. This is because we\u0026rsquo;re using JAX\u0026rsquo;s random number generator to generate a random choice, and we need to provide a key to do this.\n2. Getting choice probabilities from values choice_p = softmax(value[None, :], temperature).squeeze()\rAs we mentioned earlier, the softmax function calculates the probability of selecting each action based on its value. We pass in our estimated values for each action, and the temperature parameter, and get back a set of probabilities for selecting each action.\nBy default, the softmax function expects a 2-dimensional array of values, where the first dimension corresponds to the number of trials and the second dimension corresponds to the number of actions. However, our value array is 1-dimensional as it corresponds to the values or the current trial. We can use the None index to add an extra dimension to our array, and then squeeze to remove it again.\n‚ö†Ô∏è Note: It is important that we get choice probabilities and select actions BEFORE updating the value. When someone makes a choice on Trial 1, they are doing this without having received any information - their choice is based on their current expectation. Only after they have made a choice do they receive feedback, which is then used to update their expectation for the next trial.\n3. Choosing an action choice = choice_from_action_p(key, choice_p)\rAs we mentioned earlier, we need to pass in a random key in order to generate a random choice. We use the choice_from_action_p function to generate a random choice based on the probabilities we calculated using the softmax function.\n4. Converting to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1)\rThe choice_from_action_p function returns the index of the chosen action. We convert this index into a one-hot array, where all values are 0 except for the chosen action, which is 1. This is the format that our update function expects.\n5. Updating the estimated value updated_value = rescorla_wagner_update( value, choice_array, outcomes, alpha_p, alpha_n, )\rWe now update our estimated value based on the chosen action and the outcome of that action. We use the rescorla_wagner_update function that we defined earlier to do this.\n6. Returning useful variables return updated_value, (value, choice_p, choice_array, prediction_error)\rFinally, we return the updated value, as well as some other useful variables that we might want to keep track of, such as the choice probabilities, the one-hot array of the chosen action, and the prediction error.\nThis might seem a little odd: why do we return everything but updated_value as a tuple? We could instead do something like:\nreturn updated_value, choice_p, choice_array, prediction_error\rHowever, we will need to use this function within a jax.lax.scan loop, and jax.lax.scan expects the function to return only two values. The first value is what is fed back into the function at the next time step, and the second value is what is collected at each time step. The only variable that\u0026rsquo;s going to be reused at the next time step is updated_value, so we return this as the first value, and everything else as the second value.\nThere\u0026rsquo;s something else here that\u0026rsquo;s a bit confusing: why do we return value as well as updated_value? We don\u0026rsquo;t actually need to return value here, as we\u0026rsquo;re already returning updated_value. However, we might want to keep track of the value at each time step before it has been updated (e.g., perhaps we want to link expected value on a given trial to neural activity, in which case we want the value before it has been updated).\nTrying out the function If we try to run our function as it\u0026rsquo;s currently written, we will get an error:\n# Initialize the value, outcome, choices, and learning rates value = np.ones(5) * 0.5 outcome = np.array([1.0, 0.0, 1.0, 0.0, 1.0]) alpha_p = 0.1 alpha_n = 0.9 temperature = 0.5 # Get a random key key = jax.random.PRNGKey(0) # Call the function updated_value, (value, choice_p, choice_array, prediction_error) = asymmetric_rescorla_wagner_update_choice( value, outcome, alpha_p, alpha_n, temperature, 5, key ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Prediction Error: {prediction_error}\u0026#34;)\r--------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[46], line 12 9 key = jax.random.PRNGKey(0) 11 # Call the function ---\u0026gt; 12 updated_value, (value, choice_p, choice_array, prediction_error) = asymmetric_rescorla_wagner_update_choice( 13 value, outcome, alpha_p, alpha_n, temperature, 5, key 14 ) 16 # Print the results 17 print(f\u0026quot;Updated Value: {updated_value}\u0026quot;) [... skipping hidden 12 frame] Cell In[43], line 85, in asymmetric_rescorla_wagner_update_choice(value, outcome, alpha_p, alpha_n, temperature, n_actions, key) 82 choice = choice_from_action_p(key, choice_p) 84 # Convert it to one-hot format ---\u0026gt; 85 choice_array = jnp.zeros(n_actions, dtype=jnp.int16) 86 choice_array = choice_array.at[choice].set(1) 88 # Get the outcome and update the value estimate File ~/miniconda3/envs/transition_uncertainty/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2288, in zeros(shape, dtype) 2286 if (m := _check_forgot_shape_tuple(\u0026quot;zeros\u0026quot;, shape, dtype)): raise TypeError(m) 2287 dtypes.check_user_dtype_supported(dtype, \u0026quot;zeros\u0026quot;) -\u0026gt; 2288 shape = canonicalize_shape(shape) 2289 return lax.full(shape, 0, _jnp_dtype(dtype)) File ~/miniconda3/envs/transition_uncertainty/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:80, in canonicalize_shape(shape, context) 77 def canonicalize_shape(shape: Any, context: str=\u0026quot;\u0026quot;) -\u0026gt; core.Shape: 78 if (not isinstance(shape, (tuple, list)) and 79 (getattr(shape, 'ndim', None) == 0 or ndim(shape) == 0)): ---\u0026gt; 80 return core.canonicalize_shape((shape,), context) # type: ignore 81 else: 82 return core.canonicalize_shape(shape, context) File ~/miniconda3/envs/transition_uncertainty/lib/python3.10/site-packages/jax/_src/core.py:2130, in canonicalize_shape(shape, context) 2128 except TypeError: 2129 pass -\u0026gt; 2130 raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Traced\u0026lt;ShapedArray(int32[], weak_type=True)\u0026gt;with\u0026lt;DynamicJaxprTrace(level=1/0)\u0026gt;,). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function asymmetric_rescorla_wagner_update_choice at /tmp/ipykernel_38248/3994267310.py:45 for jit. This concrete value was not available in Python because it depends on the value of the argument n_actions. There are various clues in the error message as to what\u0026rsquo;s gone wrong:\nShapes must be 1D sequences of concrete values of integer type, got (Traced\u0026lt;ShapedArray(int32[], weak_type=True)\u0026gt;with\u0026lt;DynamicJaxprTrace(level=1/0)\u0026gt;,).\nIf using jit, try using static_argnums\nThis concrete value was not available in Python because it depends on the value of the argument n_actions.\nThis is a common problem when using JAX - we need to provide the size of our arrays at compile time, but the size of our arrays is not known until runtime.\nUsing static_argnums Essentially, the problem is that JAX needs to know the shape of everything in advance - this is partly why it can make our code run so quickly. However, in this case, the size of the choice_array variable depends upon the n_actions variable, which is not known until runtime. We\u0026rsquo;ve supplied 5 here when calling the function, but all JAX sees is an integer that could take any value.\nThe easiest solution is to tell JAX to compile the function so that it works only with the value that we\u0026rsquo;ve passed in. So we\u0026rsquo;ll get a compiled function that works for n_actions=5, but if we try to use it with n_actions=10, it will fail. This would mean that we need to recomplie the function every time we want to use it with a different number of actions, but in reality that isn\u0026rsquo;t something we\u0026rsquo;re likely to do.\nWe can do this using the static_argnums argument to jax.jit. This tells JAX that the function should be compiled with respect to the arguments that we specify. In this case, we want to compile the function with respect to the n_actions argument, so we\u0026rsquo;ll pass in 5 (the index of the n_actions argument in the function signature).\nNOTE: We need to call jax.jit() as a function rather than using it as a decorator if we want to pass in static_argnums.\ndef asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, temperature: float, n_actions: int, key: jax.random.PRNGKey, ) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function. Args: value (jax.typing.ArrayLike): The current value estimate. outcome (jax.typing.ArrayLike): The outcome of the action. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for softmax function. n_actions (int): The number of actions to choose from. key (jax.random.PRNGKey): The random key for the choice function. Returns: Tuple[np.ndarray, Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]]: - updated_value (jnp.ndarray): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]): - value (jax.typing.ArrayLike): The original value estimate. - choice_p (jnp.ndarray): The choice probabilities. - choice (int): The chosen action. - choice_array (jnp.ndarray): The chosen action in one-hot format. \u0026#34;\u0026#34;\u0026#34; # Get choice probabilities choice_p = softmax(value[None, :], temperature).squeeze() # Get choice choice = choice_from_action_p(key, choice_p) # Convert it to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1) # Get the outcome and update the value estimate updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, choice_array, outcome, alpha_p, alpha_n, ) return updated_value, (value, choice_p, choice_array, prediction_error) asymmetric_rescorla_wagner_update_choice = jax.jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5,))\rNow we can try running it again\u0026hellip;\n# Initialize the value, outcome, choices, and learning rates value = np.ones(5) * 0.5 outcome = np.array([1.0, 0.0, 1.0, 0.0, 1.0]) alpha_p = 0.1 alpha_n = 0.9 temperature = 0.5 # Get a random key key = jax.random.PRNGKey(0) # Call the function updated_value, (value, choice_p, choice_array, prediction_error) = asymmetric_rescorla_wagner_update_choice( value, outcome, alpha_p, alpha_n, temperature, 5, key ) # Print the results print(f\u0026#34;Updated Value: {updated_value}\u0026#34;) print(f\u0026#34;Choice probabilities: {choice_p}\u0026#34;) print(f\u0026#34;Choice: {choice_array}\u0026#34;)\rUpdated Value: [0.05000001 0.5 0.55 0.5 0.05000001] Choice probabilities: [0.2 0.2 0.2 0.2 0.2] Choice: [0 0 1 0 0] We can see that the function has chosen action number 2 (0-indexed), and this is the only action that has had its value updated.\n","date":"0001-01-01","id":16,"permalink":"/docs/computational_modelling/tutorial/3.-selecting-actions/","summary":"Selecting actions In most learning tasks we are asking participants to select different actions or stimuli based on their estimated value.","tags":[],"title":"3. Selecting actions"},{"content":"Updating value across trials So far, we\u0026rsquo;ve implemented a function that will select an action and update its value estimate based on the reward received for a single trial. Here, we will extend this to a sequence of trials.\nImports First, we import necessary packages.\nimport jax import numpy as np import jax.numpy as jnp from behavioural_modelling.decision_rules import softmax from behavioural_modelling.utils import choice_from_action_p from typing import Tuple import matplotlib.pyplot as plt import colormaps as cmaps import os, requests from matplotlib import font_manager, pyplot as plt # Some code to make figures look nicer url = \u0026#39;https://github.com/google/fonts/blob/main/ofl/heebo/Heebo%5Bwght%5D.ttf?raw=true\u0026#39; r = requests.get(url) if r.status_code == 200: with open(\u0026#39;./Heebo.ttf\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(r.content) font_manager.fontManager.addfont(\u0026#39;./Heebo.ttf\u0026#39;) plt.rcParams.update({\u0026#39;lines.linewidth\u0026#39;: 1, \u0026#39;lines.solid_capstyle\u0026#39;: \u0026#39;butt\u0026#39;, \u0026#39;legend.fancybox\u0026#39;: True, \u0026#39;axes.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.edgecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.subplot.left\u0026#39;: 0.08, \u0026#39;figure.subplot.right\u0026#39;: 0.95, \u0026#39;figure.subplot.bottom\u0026#39;: 0.07, \u0026#39;figure.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.dpi\u0026#39;: 80, \u0026#39;lines.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;patch.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;text.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.labelcolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;xtick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;ytick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;font.family\u0026#39;: \u0026#39;Heebo\u0026#39;, \u0026#39;font.weight\u0026#39;: \u0026#39;regular\u0026#39;, \u0026#39;font.size\u0026#39;: 12, \u0026#39;axes.titlesize\u0026#39;: 14, \u0026#39;axes.labelsize\u0026#39;: 12, \u0026#39;xtick.labelsize\u0026#39;: 10, \u0026#39;ytick.labelsize\u0026#39;: 10})\rLooping over trials using jax.lax.scan As mentioned in this guide, we can\u0026rsquo;t use a simple for loop to iterate over trials in JAX. Instead, we can use jax.lax.scan to loop over trials.\nThe scan function can be a bit confusing, but works in quite a simple way. We have a variable y that we want to update based on a sequence of inputs x, changing its value at every step. So, in our example y is the expected value, and we want to update it based on a sequence of trial outcomes (rewards), which are our x.\nWe do this based on an update function f (as we\u0026rsquo;ve been implementing so far). Our function should take two inputs: the current state of y (here, our expected value) and the current input x (here, the trial outcomes), and return two outputs: the updated state of y and any other output (e.g., prediction errors) for the current iteration.\nFor most purposes, the scan function takes three inputs:\nf: our update function, as described above. Here, this will be our Rescorla-Wagner update function. init: the initial value of y, before we update it. Here, this will be our starting expected value. xs: the sequence of inputs x that we want to iterate over. Here, this will be our trial outcomes (rewards). Rewriting our function to be slightly more JAX-friendy Here, we encounter another of JAX\u0026rsquo;s peculiarities: our function requires a random key to passed to it to randomly generate choices on each trial. This means that we will need a new random key for each trial, which means that we will need to pass in a random key sequence as an input to our function, just like we pass in our trial outcomes as a sequence.\nHowever, our function can only take two inputs: the current state of y and the current input x. Here, we want x to be both our trial outcomes and our random key sequence. To do this, we can pass in a tuple of (reward, key) as our input x, and then unpack this tuple inside our function.\n@jax.jit def asymmetric_rescorla_wagner_update( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, chosen: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0. Args: value (float): The current estimated value of a state or action. outcome (float): The actual reward received. chosen (float): Binary indicator of whether the action was chosen (1) or not (0). alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Calculate the prediction error prediction_error = outcome - value # Set prediction error to 0 for unchosen actions prediction_error = prediction_error * chosen # Set the learning rate based on the sign of the prediction error alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error def asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome_key: Tuple[jax.typing.ArrayLike, jax.random.PRNGKey], alpha_p: float, alpha_n: float, temperature: float, n_actions: int, ) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function. Args: value (jax.typing.ArrayLike): The current value estimate. outcome_key (Tuple[jax.typing.ArrayLike, jax.random.PRNGKey]): A tuple containing the outcome and the PRNG key. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for softmax function. n_actions (int): The number of actions to choose from. Returns: Tuple[np.ndarray, Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]]: - updated_value (jnp.ndarray): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]): - value (jax.typing.ArrayLike): The original value estimate. - choice_p (jnp.ndarray): The choice probabilities. - choice (int): The chosen action. - choice_array (jnp.ndarray): The chosen action in one-hot format. \u0026#34;\u0026#34;\u0026#34; # Unpack outcome and key outcome, key = outcome_key # Get choice probabilities choice_p = softmax(value[None, :], temperature).squeeze() # Get choice choice = choice_from_action_p(key, choice_p) # Convert it to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1) # Get the outcome and update the value estimate updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, choice_array, alpha_p, alpha_n, ) return updated_value, (value, choice_p, choice_array, prediction_error) asymmetric_rescorla_wagner_update_choice = jax.jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5,))\rNow, rather than having an outcome argument and a key argument, we have:\noutcome_key: Tuple[jax.typing.ArrayLike, jax.random.PRNGKey]\rThis is a tuple of (reward, key), which we can unpack inside our function as follows:\noutcome, key = outcome_key\rThis means our function now takes a single x input, as required by scan.\nFixing parameter values using partial There\u0026rsquo;s another issue we need to address. Our function takes various other parameters: alpha_p, alpha_n, temperature, and n_actions. We can\u0026rsquo;t pass these directly to scan, as our function needs to have only two inputs. We could pass them in every trial within a single tuple, as we\u0026rsquo;ve done above with the outcome and key, but this would be excessive since they don\u0026rsquo;t change across trials.\nInstead, we can use the functools.partial function to fix these parameters in our function. This will create a new function that only takes the y and x inputs, with the other parameters already set.\nfrom functools import partial # Set parameter values alpha_p = 0.1 alpha_n = 0.9 temperature = 0.5 # Use partial to create a function with fixed parameters asymmetric_rescorla_wagner_update_choice_partial = partial( asymmetric_rescorla_wagner_update_choice, alpha_p=alpha_p, alpha_n=alpha_n, temperature=temperature, n_actions=5, )\rThis gives us a new function asymmetric_rescorla_wagner_update_choice_partial that only takes the y and x inputs, with the other parameters already set. We can then pass this new function to scan.\nUsing scan Setting up the \u0026ldquo;task\u0026rdquo; First, let\u0026rsquo;s set up some outcomes for our task. We\u0026rsquo;ll start with 100 trials, and randomly generate rewards for each of our 5 stimuli such that they have different levels of rewards.\n# Number of trials N_TRIALS = 100 # Reward probabilities for each of our 5 actions reward_probs = jnp.array([0.2, 0.4, 0.3, 0.1, 0.5]) # Generate rewards for each trial for each action using Numpy # There\u0026#39;s no need to use JAX for this rng = np.random.default_rng(0) rewards = rng.binomial(n=1, p=reward_probs, size=(N_TRIALS, len(reward_probs))) # Plot the rewards plt.figure(figsize=(10, 2)) for i in range(len(reward_probs)): plt.plot(np.cumsum(rewards[:, i]), label=f\u0026#34;Action {i + 1}\u0026#34;, color=cmaps.bold(i)) plt.xlabel(\u0026#34;Trial\u0026#34;) plt.ylabel(\u0026#34;Cumulative reward\u0026#34;) plt.legend()\r\u0026lt;matplotlib.legend.Legend at 0x7fe4517fe740\u0026gt; We can see that the cumulative rewards differ across the stimuli as they each have different reward probabilities.\nRunning the model We can now use scan to apply our model to this task. We\u0026rsquo;ll start with an initial value of 0.5 for our expected value, and use some more sensible parameter values.\n# Set parameter values alpha_p = 0.1 alpha_n = 0.1 temperature = 0.1 # Use partial to create a function with fixed parameters asymmetric_rescorla_wagner_update_choice_partial = partial( asymmetric_rescorla_wagner_update_choice, alpha_p=alpha_p, alpha_n=alpha_n, temperature=temperature, n_actions=5, ) # Generate random keys using JAX rng = jax.random.PRNGKey(0) keys = jax.random.split(rng, N_TRIALS) # Initialize the value estimates value = jnp.ones(5) * 0.5 # Loop using scan _, (values, choice_ps, choices, prediction_errors) = jax.lax.scan( asymmetric_rescorla_wagner_update_choice_partial, value, (rewards, keys), )\rLet\u0026rsquo;s go through the key parts of this in a bit more detail.\nSetting up random keys rng = jax.random.PRNGKey(0) keys = jax.random.split(rng, N_TRIALS)\rWe need to set up a random key for each trial. We do this using jax.random.split to split our initial random key into a sequence of keys that is as long as the number of trials.\nRunning the scan loop _, (values, choice_ps, choices, prediction_errors) = jax.lax.scan( asymmetric_rescorla_wagner_update_choice_partial, value, (rewards, keys), )\rHere, we pass in:\nasymmetric_rescorla_wagner_update_choice_partial: our function that will be applied to each trial. value: our initial value for the expected value. (rewards, keys): our sequence of inputs for each trial. This is a tuple of (rewards, keys), where rewards is our sequence of trial outcomes and keys is our sequence of random keys. The scan function will return the two variables that our function returns. We\u0026rsquo;re only interested in the second one, which contains the values of the expected value at the start of each trial (prior to updating), the choice probabilities, the choices made, and the prediction errors. For this reason, we use _ to ignore the first output.\nPlot the output We can plot the output of our model to see how the expected value changes across trials, as well as the choice probabilities, choices made, and prediction errors.\nFirst, we\u0026rsquo;ll plot the expected value across trials.\n# Plot the estimated values plt.figure(figsize=(10, 2)) for i in range(len(reward_probs)): plt.plot(values[:, i], label=f\u0026#34;Action {i + 1}\u0026#34;, color=cmaps.bold(i)) plt.xlabel(\u0026#34;Trial\u0026#34;) plt.ylabel(\u0026#34;Expected value\u0026#34;) plt.legend()\r\u0026lt;matplotlib.legend.Legend at 0x7fe4518dce20\u0026gt; As expected, the action with the highest value has the highest estimated value. This is a bit messy as the model will continue to respond to prediction errors and update its value estimate throughout the task.\n","date":"0001-01-01","id":17,"permalink":"/docs/computational_modelling/tutorial/4.-updating-value-across-trials/","summary":"Updating value across trials So far, we\u0026rsquo;ve implemented a function that will select an action and update its value estimate based on the reward received for a single trial.","tags":[],"title":"4. Updating value across trials"},{"content":"Updating value across subjects The model we\u0026rsquo;ve implemented so far works for a single subject, but we will typically want to run it for multiple subjects.\nImports First, we import necessary packages.\nimport jax import numpy as np import jax.numpy as jnp from functools import partial from behavioural_modelling.decision_rules import softmax from behavioural_modelling.utils import choice_from_action_p from typing import Tuple import matplotlib.pyplot as plt import colormaps as cmaps import os, requests from matplotlib import font_manager, pyplot as plt # Some code to make figures look nicer url = \u0026#39;https://github.com/google/fonts/blob/main/ofl/heebo/Heebo%5Bwght%5D.ttf?raw=true\u0026#39; r = requests.get(url) if r.status_code == 200: with open(\u0026#39;./Heebo.ttf\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(r.content) font_manager.fontManager.addfont(\u0026#39;./Heebo.ttf\u0026#39;) plt.rcParams.update({\u0026#39;lines.linewidth\u0026#39;: 1, \u0026#39;lines.solid_capstyle\u0026#39;: \u0026#39;butt\u0026#39;, \u0026#39;legend.fancybox\u0026#39;: True, \u0026#39;axes.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.edgecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.subplot.left\u0026#39;: 0.08, \u0026#39;figure.subplot.right\u0026#39;: 0.95, \u0026#39;figure.subplot.bottom\u0026#39;: 0.07, \u0026#39;figure.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.dpi\u0026#39;: 80, \u0026#39;lines.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;patch.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;text.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.labelcolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;xtick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;ytick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;font.family\u0026#39;: \u0026#39;Heebo\u0026#39;, \u0026#39;font.weight\u0026#39;: \u0026#39;regular\u0026#39;, \u0026#39;font.size\u0026#39;: 12, \u0026#39;axes.titlesize\u0026#39;: 14, \u0026#39;axes.labelsize\u0026#39;: 12, \u0026#39;xtick.labelsize\u0026#39;: 10, \u0026#39;ytick.labelsize\u0026#39;: 10})\rFunctions from the previous section Here we\u0026rsquo;ll copy the functions we\u0026rsquo;ve implemented in the previous sections.\n@jax.jit def asymmetric_rescorla_wagner_update( value: jax.typing.ArrayLike, outcome: jax.typing.ArrayLike, chosen: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0. Args: value (float): The current estimated value of a state or action. outcome (float): The actual reward received. chosen (float): Binary indicator of whether the action was chosen (1) or not (0). alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Calculate the prediction error prediction_error = outcome - value # Set prediction error to 0 for unchosen actions prediction_error = prediction_error * chosen # Set the learning rate based on the sign of the prediction error alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error def asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome_key: Tuple[jax.typing.ArrayLike, jax.random.PRNGKey], alpha_p: float, alpha_n: float, temperature: float, n_actions: int, ) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function. Args: value (jax.typing.ArrayLike): The current value estimate. outcome_key (Tuple[jax.typing.ArrayLike, jax.random.PRNGKey]): A tuple containing the outcome and the PRNG key. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for softmax function. n_actions (int): The number of actions to choose from. Returns: Tuple[np.ndarray, Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]]: - updated_value (jnp.ndarray): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]): - value (jax.typing.ArrayLike): The original value estimate. - choice_p (jnp.ndarray): The choice probabilities. - choice (int): The chosen action. - choice_array (jnp.ndarray): The chosen action in one-hot format. \u0026#34;\u0026#34;\u0026#34; # Unpack outcome and key outcome, key = outcome_key # Get choice probabilities choice_p = softmax(value[None, :], temperature).squeeze() # Get choice choice = choice_from_action_p(key, choice_p) # Convert it to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1) # Get the outcome and update the value estimate updated_value, prediction_error = asymmetric_rescorla_wagner_update( value, outcome, choice_array, alpha_p, alpha_n, ) return updated_value, (value, choice_p, choice_array, prediction_error) asymmetric_rescorla_wagner_update_choice = jax.jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5,))\rTurning our existing code into a function We had previously written some code to run our update function across multiple trials using jax.lax.scan. Here, we will turn this code into a function that can be called for multiple subjects.\ndef asymmetric_rescorla_wagner_update_choice_iterator( outcomes: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, temperature: float, n_actions: int, key: jax.random.PRNGKey, ) -\u0026gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]: \u0026#34;\u0026#34;\u0026#34; Updates the value estimates using the asymmetric Rescorla-Wagner algorithm and generates choices for each trial. Args: outcomes (jax.typing.ArrayLike): The outcomes for each trial. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for the softmax function. n_actions (int): The number of actions to choose from. key (jax.random.PRNGKey): The random key. Returns: Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]: - values (jnp.ndarray): The value estimates. - choice_ps (jnp.ndarray): The choice probabilities. - choices (jnp.ndarray): The chosen actions. - prediction_errors (jnp.ndarray): The prediction errors. \u0026#34;\u0026#34;\u0026#34; # Use partial to create a function with fixed parameters asymmetric_rescorla_wagner_update_choice_partial = partial( asymmetric_rescorla_wagner_update_choice, alpha_p=alpha_p, alpha_n=alpha_n, temperature=temperature, n_actions=n_actions, ) # Generate random keys using JAX keys = jax.random.split(key, N_TRIALS) # Initialize the value estimates value = jnp.ones(5) * 0.5 # Loop using scan _, (values, choice_ps, choices, prediction_errors) = jax.lax.scan( asymmetric_rescorla_wagner_update_choice_partial, value, (outcomes, keys), ) return values, choice_ps, choices, prediction_errors # JIT asymmetric_rescorla_wagner_update_choice_iterator = jax.jit(asymmetric_rescorla_wagner_update_choice_iterator, static_argnums=(4,))\rWe can then use our function as before to simulate data for a single subject.\n# Number of trials N_TRIALS = 100 # Reward probabilities for each of our 5 actions reward_probs = jnp.array([0.2, 0.4, 0.3, 0.1, 0.5]) # Generate rewards for each trial for each action using Numpy # There\u0026#39;s no need to use JAX for this rng = np.random.default_rng(0) rewards = rng.binomial(n=1, p=reward_probs, size=(N_TRIALS, len(reward_probs))) # Run the model (values, choice_ps, choices, prediction_errors) = asymmetric_rescorla_wagner_update_choice_iterator( outcomes=rewards, alpha_p=0.1, alpha_n=0.2, temperature=1.0, n_actions=5, key=jax.random.PRNGKey(0), )\rExtending to multiple subjects Extending to multiple subjects is straightforward thanks to JAX\u0026rsquo;s vectorization capabilities. We can use the vmap function to run our model for multiple subjects in parallel.\nWhen using vmap, we supply the function we want to run for each subject, and the axis along which we want to run it. For example, if we have a set of trial outcomes represented by a 2-dimensional array where the first dimension indexes the subject and the second dimension indexes the trial, we can run our model for each subject by setting axis=0.\nHere, we will create a vmap-ed version of our function. We will assume that the trial outcomes are the same for each subject, and that the parameters are different for each subject.\nasymmetric_rescorla_wagner_update_choice_iterator_vmap = jax.vmap( asymmetric_rescorla_wagner_update_choice_iterator, in_axes=(None, 0, 0, 0, None, None), )\rThe key part here is the in_axes argument to vmap. This tells JAX which arguments to the function are different across subjects. In our case, the only arguments that change across subjects are the parameters, which are vectors with a single dimensions. We therefore set the corresponding value of in_axes to 0 for these arguments. The others are set to None, which indicates that they are the same for all subjects.\nRunning the model for multiple subjects We can now take our vmap-ed function and run it for multiple subjects. We will simulate data for 40 subjects, each with 100 trials.\n# Number of subjects N_SUBJECTS = 40 # Generate parameter values for each subject alpha_p = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) alpha_n = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) temperature = np.random.uniform(0.1, 1.0, size=N_SUBJECTS) # Run the model for each subject # NOTE - this doesn\u0026#39;t work if you pass keyword arguments values, choice_ps, choices, prediction_errors = asymmetric_rescorla_wagner_update_choice_iterator_vmap( rewards, alpha_p, alpha_n, temperature, 5, jax.random.PRNGKey(0), )\rWe can then plot the estimated value for a few subjects to see how they learn over time.\n# Plot the estimated values for the first 3 subjects, one subplot per subject fig, axs = plt.subplots(3, 1, figsize=(10, 5)) for i in range(3): axs[i].plot(values[i, :]) axs[i].set_title(f\u0026#34;Subject {i + 1}\u0026#34;) if i == 2: axs[i].set_xlabel(\u0026#34;Action\u0026#34;) axs[i].set_ylabel(\u0026#34;Estimated Value\u0026#34;) plt.tight_layout()\rExtending further One of the nice things about the vmap functionality is that it is easily to extend. If we wanted to, we could add further dimensions to map across. For example, we could map across blocks of trials, or different conditions. This enables us to use our base update functions flexibly without losing performance.\nFor example, we could first map across blocks, assuming our outcomes variable is a 2-dimensional array with the shape (n_blocks, n_trials):\nasymmetric_rescorla_wagner_update_choice_iterator_vmap_blocks = jax.vmap( asymmetric_rescorla_wagner_update_choice_iterator, in_axes=(0, None, None, None, None, None), )\rHere, we would set in_axes=(0, None, None, None, None, None) because the first argument (the outcomes) changes across blocks, but the others do not (i.e., the parameter values are the same for each block within each subject).\nWe could then map this function across subjects as before:\nasymmetric_rescorla_wagner_update_choice_iterator_vmap_blocks_subjects = jax.vmap( asymmetric_rescorla_wagner_update_choice_iterator_vmap_blocks, in_axes=(0, None, None, None, None, None), )\rDemonstrating why JAX is helpful A lot of this imlementation has probably seemed a little convoluted. However, the key advantage of using JAX is that it allows us to write code that is both flexible and efficient. By using vmap, we can write our base update functions in a way that is easy to understand and debug, and then use them in a flexible way without losing performance. This is a powerful feature that is unique to JAX.\nSpeed isn\u0026rsquo;t necessarily a huge issue when we\u0026rsquo;re dealing with small samples of participants, but we\u0026rsquo;re increasingly using datasets with hundreds of participants or more. In these cases, the efficiency of JAX can be a huge advantage, especially if we\u0026rsquo;re running model fitting procedures that require many iterations (e.g., MCMC sampling).\nThe vectorised operations we\u0026rsquo;ve used here (using vmap) are much faster than using for loops in Python. These operations can be sped up even further when run on a GPU, which JAX also supports without any need to change the code (if JAX detects a CUDA-compatible GPU, it will automatically run the code on the GPU). This can lead to huge speedups, especially for large datasets.\nTo demonstrate this, we can compare the speed of our JAX implementation to a naive implementation that uses for loops to iterate over subjects and trials. We\u0026rsquo;ll simulate data for 10000 subjects, each with 100 trials, and compare the speed of the two implementations.\nN_SUBJECTS = 10000 def rescorla_wagner_update(value, outcome, chosen, alpha_p, alpha_n): # Calculate the prediction error prediction_error = outcome - value # Set unchosen actions to 0 prediction_error = prediction_error * chosen # Get the alpha for this trial alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value value = value + alpha_t * prediction_error return value, prediction_error def softmax(value, temperature): # Calculate the unnormalized probabilities unnormalized_p = np.exp(value / temperature) # Calculate the denominator normalizing_constant = np.sum(unnormalized_p) # Calculate the probabilities p = unnormalized_p / normalizing_constant return p def test_pure_python(): values = np.ones((N_SUBJECTS, N_TRIALS, 5)) * 0.5 # RNG rng = np.random.default_rng(0) # Generate parameter values for each subject alpha_p = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) alpha_n = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) temperature = np.random.uniform(0.1, 1.0, size=N_SUBJECTS) for subject in range(len(alpha_p)): for trial in range(N_TRIALS - 1): # Get the value estimate for the current trial value = values[subject, trial] # Get choice probabilities choice_p = softmax(value, temperature[subject]) # Choose an action choice = rng.choice(np.arange(5), p=choice_p) # Convert to one-hot choice_array = np.zeros(5) choice_array[choice] = 1 # Update the value estimate using the Rescorla-Wagner learning rule value, pe = rescorla_wagner_update(value, rewards[trial], choice_array, alpha_p[subject], alpha_n[subject]) # Store the updated value estimate values[subject, trial] = value def test_jax(): # Generate parameter values for each subject alpha_p = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) alpha_n = np.random.uniform(0.1, 0.5, size=N_SUBJECTS) temperature = np.random.uniform(0.1, 1.0, size=N_SUBJECTS) # Run the model for each subject # NOTE - this doesn\u0026#39;t work if you pass keyword arguments values, choice_ps, choices, prediction_errors = asymmetric_rescorla_wagner_update_choice_iterator_vmap( rewards, alpha_p, alpha_n, temperature, 5, jax.random.PRNGKey(0), )\rFirst, we\u0026rsquo;ll test the pure Python/Numpy implementation.\n%time test_pure_python()\rCPU times: user 25.5 s, sys: 19.3 ms, total: 25.5 s Wall time: 25.5 s And next we\u0026rsquo;ll test the JAX implementation.\n%time test_jax()\rCPU times: user 232 ms, sys: 327 ms, total: 559 ms Wall time: 281 ms On my machine, the pure Python/Numpy implementation takes around 25 seconds, while the JAX implementation takes around 0.2 seconds. This is a huge speedup (around 100x), and the difference will be even more pronounced for larger datasets.\n","date":"0001-01-01","id":18,"permalink":"/docs/computational_modelling/tutorial/5.-running-the-model-for-multiple-subjects/","summary":"Updating value across subjects The model we\u0026rsquo;ve implemented so far works for a single subject, but we will typically want to run it for multiple subjects.","tags":[],"title":"5. Running the model for multiple subjects"},{"content":"Model fitting using MCMC Next, we\u0026rsquo;ll try fitting our model to some simulated data.\nImports First, we import necessary packages.\nimport numpyro numpyro.set_host_device_count(4) # Necessary to make numpyro realise we have more than one CPU import jax # set jax to use cpu jax.config.update(\u0026#39;jax_platform_name\u0026#39;, \u0026#39;cpu\u0026#39;) import numpy as np import jax.numpy as jnp from numpyro import distributions as dist from functools import partial from behavioural_modelling.decision_rules import softmax from behavioural_modelling.utils import choice_from_action_p from typing import Tuple, Union import matplotlib.pyplot as plt # import colormaps as cmaps import os, requests # from matplotlib import font_manager, pyplot as plt # # Some code to make figures look nicer # url = \u0026#39;https://github.com/google/fonts/blob/main/ofl/heebo/Heebo%5Bwght%5D.ttf?raw=true\u0026#39; # r = requests.get(url) # if r.status_code == 200: # with open(\u0026#39;./Heebo.ttf\u0026#39;, \u0026#39;wb\u0026#39;) as f: f.write(r.content) # font_manager.fontManager.addfont(\u0026#39;./Heebo.ttf\u0026#39;) # plt.rcParams.update({\u0026#39;lines.linewidth\u0026#39;: 1, \u0026#39;lines.solid_capstyle\u0026#39;: \u0026#39;butt\u0026#39;, \u0026#39;legend.fancybox\u0026#39;: True, \u0026#39;axes.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.edgecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;savefig.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.subplot.left\u0026#39;: 0.08, \u0026#39;figure.subplot.right\u0026#39;: 0.95, \u0026#39;figure.subplot.bottom\u0026#39;: 0.07, \u0026#39;figure.facecolor\u0026#39;: \u0026#39;fafafa\u0026#39;, \u0026#39;figure.dpi\u0026#39;: 80, \u0026#39;lines.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;patch.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;text.color\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.edgecolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;axes.labelcolor\u0026#39;: \u0026#39;383838\u0026#39;, \u0026#39;xtick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;ytick.color\u0026#39;: \u0026#39;616161\u0026#39;, \u0026#39;font.family\u0026#39;: \u0026#39;Heebo\u0026#39;, \u0026#39;font.weight\u0026#39;: \u0026#39;regular\u0026#39;, \u0026#39;font.size\u0026#39;: 12, \u0026#39;axes.titlesize\u0026#39;: 14, \u0026#39;axes.labelsize\u0026#39;: 12, \u0026#39;xtick.labelsize\u0026#39;: 10, \u0026#39;ytick.labelsize\u0026#39;: 10})\rFunctions from the previous section Here we\u0026rsquo;ll (mostly) copy the functions we\u0026rsquo;ve implemented in the previous sections.\n‚ö†Ô∏è One important change We\u0026rsquo;re going to update the basic asymmetric_rescorla_wagner_update function so that it is more compatible with jax.lax.scan, as we\u0026rsquo;ll need to use this later.\nRather than taking separate outcome and chosen arguments, representing the reward received on the current trial and the action chosen on the current trial, we\u0026rsquo;ll instead take a single outcome_chosen argument, which is a tuple of (outcome, chosen). As mentioned in previous sections, this is a common pattern in JAX code, as it allows us to use jax.lax.scan more easily (since it only supports functions with a single input argument).\nWe\u0026rsquo;ll also update its return values, so that it returns the updated_value and a tuple of (prediction_error, value), rather than just value. This will make it easier to use with jax.lax.scan as well.\n@jax.jit def asymmetric_rescorla_wagner_update( value: jax.typing.ArrayLike, outcome_chosen: Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike], alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: \u0026#34;\u0026#34;\u0026#34; Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule. The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative. Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0. Args: value (float): The current estimated value of a state or action. outcome_chosen (Tuple[float, float]): A tuple containing the actual outcome and a binary value indicating whether the action was chosen. alpha_p (float): The learning rate used when the prediction error is positive. alpha_n (float): The learning rate used when the prediction error is negative. Returns: Tuple[float, float]: The updated value and the prediction error. \u0026#34;\u0026#34;\u0026#34; # Unpack the outcome and the chosen action outcome, chosen = outcome_chosen # Calculate the prediction error prediction_error = outcome - value # Set prediction error to 0 for unchosen actions prediction_error = prediction_error * chosen # Set the learning rate based on the sign of the prediction error alpha_t = (alpha_p * (prediction_error \u0026gt; 0)) + (alpha_n * (prediction_error \u0026lt; 0)) # Update the value updated_value = value + alpha_t * prediction_error return updated_value, (value, prediction_error) def asymmetric_rescorla_wagner_update_choice( value: jax.typing.ArrayLike, outcome_key: Tuple[jax.typing.ArrayLike, jax.random.PRNGKey], alpha_p: float, alpha_n: float, temperature: float, n_actions: int, ) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function. Args: value (jax.typing.ArrayLike): The current value estimate. outcome_key (Tuple[jax.typing.ArrayLike, jax.random.PRNGKey]): A tuple containing the outcome and the PRNG key. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for softmax function. n_actions (int): The number of actions to choose from. Returns: Tuple[np.ndarray, Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]]: - updated_value (jnp.ndarray): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, np.ndarray, int, np.ndarray]): - value (jax.typing.ArrayLike): The original value estimate. - choice_p (jnp.ndarray): The choice probabilities. - choice (int): The chosen action. - choice_array (jnp.ndarray): The chosen action in one-hot format. \u0026#34;\u0026#34;\u0026#34; # Unpack outcome and key outcome, key = outcome_key # Get choice probabilities choice_p = softmax(value[None, :], temperature).squeeze() # Get choice choice = choice_from_action_p(key, choice_p) # Convert it to one-hot format choice_array = jnp.zeros(n_actions, dtype=jnp.int16) choice_array = choice_array.at[choice].set(1) # Get the outcome and update the value estimate updated_value, (value, prediction_error) = asymmetric_rescorla_wagner_update( value, (outcome, choice_array), alpha_p, alpha_n, ) return updated_value, (value, choice_p, choice_array, prediction_error) asymmetric_rescorla_wagner_update_choice = jax.jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5,)) def asymmetric_rescorla_wagner_update_choice_iterator( outcomes: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, temperature: float, n_actions: int, key: jax.random.PRNGKey, ) -\u0026gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]: \u0026#34;\u0026#34;\u0026#34; Updates the value estimates using the asymmetric Rescorla-Wagner algorithm and generates choices for each trial. Args: outcomes (jax.typing.ArrayLike): The outcomes for each trial. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. temperature (float): The temperature parameter for the softmax function. n_actions (int): The number of actions to choose from. key (jax.random.PRNGKey): The random key. Returns: Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]: - values (jnp.ndarray): The value estimates. - choice_ps (jnp.ndarray): The choice probabilities. - choices (jnp.ndarray): The chosen actions. - prediction_errors (jnp.ndarray): The prediction errors. \u0026#34;\u0026#34;\u0026#34; # Use partial to create a function with fixed parameters asymmetric_rescorla_wagner_update_choice_partial = partial( asymmetric_rescorla_wagner_update_choice, alpha_p=alpha_p, alpha_n=alpha_n, temperature=temperature, n_actions=n_actions, ) # Generate random keys using JAX keys = jax.random.split(key, N_TRIALS) # Initialize the value estimates value = jnp.ones(5) * 0.5 # Loop using scan _, (values, choice_ps, choices, prediction_errors) = jax.lax.scan( asymmetric_rescorla_wagner_update_choice_partial, value, (outcomes, keys), ) return values, choice_ps, choices, prediction_errors # JIT asymmetric_rescorla_wagner_update_choice_iterator = jax.jit(asymmetric_rescorla_wagner_update_choice_iterator, static_argnums=(4,)) asymmetric_rescorla_wagner_update_choice_iterator_vmap = jax.vmap( asymmetric_rescorla_wagner_update_choice_iterator, in_axes=(None, 0, 0, 0, None, None), )\rSimulate some data We\u0026rsquo;ll start by simulating some data from the model we\u0026rsquo;ve defined. We\u0026rsquo;ll generate some parameter values for each of our simulated \u0026ldquo;subjects\u0026rdquo;, and then generate some simulated choices for each subject.\nWe\u0026rsquo;re going to generate parameter values using appropriate distributions. The alpha_p and alpha_n parameters will be drawn from a Beta distribution since their values lie between 0 and 1, while the temperature parameter will be drawn from an exponential distribution. This is because the temperature parameter is always positive, and the exponential distribution is a good choice for positive-valued parameters.\nNOTE: We don\u0026rsquo;t need any of the other variables returned by the model here, so we\u0026rsquo;ll just ignore them.\n# Number of subjects N_SUBJECTS = 40 # Generate parameter values for each subject rng = np.random.default_rng(0) alpha_p = rng.beta(5, 5, size=N_SUBJECTS) alpha_n = rng.beta(5, 5, size=N_SUBJECTS) temperature = rng.beta(5, 5, size=N_SUBJECTS) # Number of trials N_TRIALS = 200 # Reward probabilities for each of our 5 actions reward_probs = jnp.array([0.2, 0.4, 0.8, 0.1, 0.5]) # Generate rewards for each trial for each action using Numpy # There\u0026#39;s no need to use JAX for this rng = np.random.default_rng(0) rewards = rng.binomial(n=1, p=reward_probs, size=(N_TRIALS, len(reward_probs))) # Run the model for each subject _, _, choices, _ = asymmetric_rescorla_wagner_update_choice_iterator_vmap( rewards, alpha_p, alpha_n, temperature, 5, jax.random.PRNGKey(0), )\rSet up another function for model fitting Our current implementation simulates choices for each trial based on the current expected value. However, when we\u0026rsquo;re fitting the model we won\u0026rsquo;t want to do this. This is because we want to fit the model to the actual choices made by the subject, rather than the choices that the model would make (you could do this, but it generally works poorly) - we\u0026rsquo;re essentially saying \u0026ldquo;given this option was chosen on the previous trial, how close is the model\u0026rsquo;s prediction for the next choice to the actual choice made?\u0026rdquo;.\nThis means we want our model to return the expected value for each option on each trial, given the observed choice, rather than generating a choice and updating based on that choice.\nFor this reason, we\u0026rsquo;ll use our original asymmetric_rescorla_wagner_update function and do the same as before to make it run across all trials for all subjects. As mentioned above, we\u0026rsquo;ve modified this function slightly to make it more compatible with jax.lax.scan.\nWe can ignore the softmax function here as we don\u0026rsquo;t need to generate choices for each trial sequentially, and this can be applied to our estimated values at the end.\n@jax.jit def asymmetric_rescorla_wagner_update_iterator( outcomes: jax.typing.ArrayLike, choices: jax.typing.ArrayLike, alpha_p: float, alpha_n: float, ) -\u0026gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]: \u0026#34;\u0026#34;\u0026#34; Updates the value estimates using the asymmetric Rescorla-Wagner algorithm. Args: outcomes (jax.typing.ArrayLike): The outcomes for each trial. alpha_p (float): The learning rate for positive outcomes. alpha_n (float): The learning rate for negative outcomes. Returns: Tuple[jnp.ndarray, jnp.ndarray]: - values (jnp.ndarray): The value estimates. - prediction_errors (jnp.ndarray): The prediction errors. \u0026#34;\u0026#34;\u0026#34; # Use partial to create a function with fixed parameters asymmetric_rescorla_wagner_update_partial = partial( asymmetric_rescorla_wagner_update, alpha_p=alpha_p, alpha_n=alpha_n, ) # Initialize the value estimates value = jnp.ones(5) * 0.5 # Loop using scan _, (values, prediction_errors) = jax.lax.scan( asymmetric_rescorla_wagner_update_partial, value, (outcomes, choices), ) return values, prediction_errors asymmetric_rescorla_wagner_update_iterator_vmap = jax.vmap( asymmetric_rescorla_wagner_update_iterator, in_axes=(None, 0, 0, 0), )\rIn contrast to the functions above which randomly make choices on each trial, we\u0026rsquo;re passing in choices as an argument, which is a (n_trials, n_actions) array of choices made on each trial.\nOur vmap function will then apply this function to each subject, and we can use this to calculate the log likelihood of the data given the model parameters. As before, the rewards are the same for every subject, so we set the corresponding value for in_axes to None. The remaining arguments are the choices and parameters, which differ for every subject - for this reason, we set the corresponding value for in_axes to 0.\nAs with our other vmap-ed functions, we can then use this to simulate expected values for each trial for each subject. We don\u0026rsquo;t need the prediction errors here, so we\u0026rsquo;ll only assign the expected values to a variable (assigning the prediction errors to _).\n# Run the model for each subject values, _ = asymmetric_rescorla_wagner_update_iterator_vmap( rewards, choices, alpha_p, alpha_n, )\rApplying the softmax function As mentioned above, we don\u0026rsquo;t need to apply the softmax function to generate choices, but we will need to apply it to our expected values to calculate the probability of the observed choices given the data. We can do this by applying the softmax function to the expected values for each subject once they have been calculated, as we don\u0026rsquo;t need them to be calculated for each trial sequentially.\nThe softmax function is designed to calculate choice probabilities for a single subject, we we\u0026rsquo;ll again use vmap to map it over subjects.\nchoice_p = jax.vmap(softmax, in_axes=(0, 0))(values, temperature)\rSet up the statistical model As described in this guide, the model we\u0026rsquo;ve implemented so far is our behavioural model - it generates simulated behaviour based on a set of parameters. We want to estimate parameter values for this model for each subject in our dataset - to do this we need to set up a statistical model, which describes how we think the parameters are distributed across subjects.\nWe will do this using NumPyro, which is a probabilistic programming library built on top of JAX. This will allow us to use MCMC to estimate the parameters of our model.\nBuilding a hierarchical model for our parameters We will use a hiearchical modelling approach using a non-centred parameterisation (as described in this guide) to estimate the parameters of our model. This assumes that each subject is drawn from a group rather than being completely independent, and uses the information from the group to improve the estimates of the individual subjects.\nWe have three parameters that we want to estimate:\nalpha_p - the learning rate for positive prediction errors. This parameter takes values between 0 and 1. alpha_n - the learning rate for negative prediction errors. This parameter takes values between 0 and 1. temperature - the temperature parameter for the softmax function. This parameter takes values between 0 and infinity. It is often easiest to estimate parameters whose values lie within clear ranges, whereas the softmax temperature parameter can take any positive value. To make this easier to estimate, we can sample within the range 0-1 and then transform it to the range 0-infinity using the reciprocal function:\n$$ \\text{temperature} = \\frac{1}{\\text{temperature}} $$ Create a function for generating non-centred parameterisations We need to set up each parameter using a non-centred parameterisation, which means that each parameter is composed of three separate components:\nA group-level parameter mean mu which is drawn from a normal distribution with mean 0 and standard deviation 1, with one value for all subjects. A group-level parameter standard deviation sigma, which is drawn from a half-normal distribution with a scale parameter of 1, with one value for all subjects. A subject-level parameter offset which is drawn from a normal distribution with mean 0 and standard deviation 1, with one value for each subject. The subject-level parameter is then calculated as:\nsubject_parameter = mu + sigma * offset\rCreating these componenets for every parameter can be quite repetitive, so we\u0026rsquo;ll create a function to do this for us.\nimport numpyro def create_subject_params( name: str, n_subs: int ) -\u0026gt; Union[dist.Normal, dist.HalfNormal, dist.Normal]: \u0026#34;\u0026#34;\u0026#34; Creates group mean, group sd and subject-level offset parameters. Args: name (str): Name of the parameter n_subs (int): Number of subjects Returns: Union[dist.Normal, dist.HalfNormal, dist.Normal]: Group mean, group sd, and subject-level offset parameters \u0026#34;\u0026#34;\u0026#34; # Group-level mean and SD group_mean = numpyro.sample(\u0026#34;{0}_group_mean\u0026#34;.format(name), dist.Normal(0, 1)) group_sd = numpyro.sample(\u0026#34;{0}_group_sd\u0026#34;.format(name), dist.HalfNormal(1)) # Subject-level offset offset = numpyro.sample( \u0026#34;{0}_subject_offset\u0026#34;.format(name), dist.Normal(0, 1), sample_shape=(n_subs,) # One value per subject ) # Calculate subject-level parameter subject_param = numpyro.deterministic(\u0026#34;{0}_subject_param\u0026#34;.format(name), group_mean + offset * group_sd) return subject_param\rThis function generates the three components of the non-centred parameterisation for a single parameter, given the name of the parameter and the number of subjects. It then uses these to form the subject-level parameter value for each subject.\nWe use numpyro.deterministic to create a deterministic variable in the model, which is a variable that is not sampled from, but is instead calculated from other variables in the model. This is useful for creating variables that are derived from other variables in the model, but that we don\u0026rsquo;t want to sample from directly. We could instead do something like:\nreturn group_mean + offset * group_std\rBut this would mean that the subject-level parameter wouldn\u0026rsquo;t actually be stored, and this is the parameter whose value we actually care about.\nWe could also adjust the priors on these parameters, but these values should all work reasonably well for our purposes.\nBounding our parameters As it stands, this approach will generate parameters that are unbounded, which is not ideal for our purposes as all of our parameters will be estimated in the range 0-1. One way to do this is to use a transforamtion that brings the parameters into the range 0-1. This is a common approach in Bayesian modelling, as it allows us to estimate parameters in an unbounded space, but then transform them to a bounded space.\nWe can add this to the function using the jax.scipy.special.expit function, which is the inverse of the logit function. This function transforms any value to the range 0-1, which is perfect for our purposes.\ndef create_subject_params( name: str, n_subs: int ) -\u0026gt; Union[dist.Normal, dist.HalfNormal, dist.Normal]: \u0026#34;\u0026#34;\u0026#34; Creates group mean, group sd and subject-level offset parameters. Args: name (str): Name of the parameter n_subs (int): Number of subjects Returns: jnp.array: Subject-level parameter for each subject \u0026#34;\u0026#34;\u0026#34; # Group-level mean and SD group_mean = numpyro.sample(\u0026#34;{0}_group_mean\u0026#34;.format(name), dist.Normal(0, 0.5)) group_sd = numpyro.sample(\u0026#34;{0}_group_sd\u0026#34;.format(name), dist.HalfNormal(0.5)) # Subject-level offset offset = numpyro.sample( \u0026#34;{0}_subject_offset\u0026#34;.format(name), dist.Normal(0, 0.5), sample_shape=(n_subs,), # One value per subject ) # Calculate subject-level parameter subject_param = numpyro.deterministic( \u0026#34;{0}_subject_param\u0026#34;.format(name), jax.scipy.special.expit(group_mean + offset * group_sd), ) return subject_param\rThe subject-level parameter is now calculated as:\njax.scipy.special.expit(group_mean + offset * group_sd)\rAs a quick demonstration of how this works, we can generate some random values and transform them. We can see that the transformed values are all between 0 and 1.\n# Sample from a normal distribution rng = np.random.default_rng(0) x = rng.normal(0, 1, size=1000) # Plot the histogram of the untransformed data plt.hist(x) # Plot the histogram of the transformed data plt.hist(jax.scipy.special.expit(x), bins=30)\r(array([ 6., 5., 7., 14., 19., 28., 43., 46., 32., 46., 50., 55., 51., 57., 52., 45., 49., 43., 53., 44., 36., 50., 27., 36., 39., 22., 20., 13., 9., 3.]), array([0.01985156, 0.05103884, 0.08222611, 0.11341339, 0.14460067, 0.17578794, 0.20697522, 0.2381625 , 0.26934978, 0.30053705, 0.33172435, 0.36291161, 0.39409891, 0.42528617, 0.45647344, 0.48766074, 0.518848 , 0.5500353 , 0.58122259, 0.61240983, 0.64359713, 0.67478442, 0.70597166, 0.73715895, 0.76834625, 0.79953349, 0.83072078, 0.86190808, 0.89309537, 0.92428261, 0.95546991]), \u0026lt;BarContainer object of 30 artists\u0026gt;) Putting the model together We can now write a function that encapsulates the statistical model for our parameters:\ndef asymmetric_rescorla_wagner_statistical_model( outcomes: jnp.ndarray, choices: jnp.ndarray, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Asymmetric Rescorla-Wagner model for NumPyro. This forms a hierarchical model using non-centred parameterisation. Args: outcomes (jnp.ndarray): The outcomes for each trial. choices (jnp.ndarray): The choices for each trial. Returns: None: The function does not return anything; it only samples from the model. \u0026#34;\u0026#34;\u0026#34; # Get number of subjects based on choices n_subs = choices.shape[0] # Create subject-level parameters alpha_p = create_subject_params(\u0026#34;alpha_p\u0026#34;, n_subs) alpha_n = create_subject_params(\u0026#34;alpha_n\u0026#34;, n_subs) temperature = create_subject_params(\u0026#34;temperature\u0026#34;, n_subs) # Run the model for each subject values, _ = asymmetric_rescorla_wagner_update_iterator_vmap( rewards, choices, alpha_p, alpha_n, ) # Get choice probabilities using inverse temperature choice_p = jax.vmap(softmax, in_axes=(0, 0))(values, temperature) # Bernoulli likelihood numpyro.sample( \u0026#34;observed_choices\u0026#34;, dist.Bernoulli(probs=choice_p), obs=choices, )\rLet\u0026rsquo;s break this down a bit:\n1. We enter the outcomes and observed choices def asymmetric_rescorla_wagner_statistical_model( outcomes: jnp.ndarray, choices: jnp.ndarray, ) -\u0026gt; None:\rWe provide the observed outcomes, which are used by our model to calculate prediction errors and update expected value estimates. We also provide the observed choices, which are the choices made by the subject on each trial that we use to calculate the likelihood of the data given the model.\n2. We set up priors for our parameters alpha_p = create_subject_params(\u0026#34;alpha_p\u0026#34;, n_subs) alpha_n = create_subject_params(\u0026#34;alpha_n\u0026#34;, n_subs) temperature = create_subject_params(\u0026#34;temperature\u0026#34;, n_subs)\rThe priors on our parameters are generated using the function we defined above to create non-centred parameterisations for each parameter. All of these are estimated in the range 0-1.\n3. We calculate the expected values for each subject Next, we feed these parameters, along with the subjects\u0026rsquo; choices, into our behavioural model to estimate expected values for each subject. We use the vmap-ed version of our model to apply it to each subject.\nvalues, _ = asymmetric_rescorla_wagner_update_iterator_vmap( rewards, choices, alpha_p, alpha_n, )\r4. We apply the softmax function to the expected values choice_p = jax.vmap(softmax, in_axes=(0, 0))(values, 1 / temperature)\rWe then apply the softmax function to the expected values for each subject, using the temperature parameter for each subject. This gives us the probability of each action being chosen on each trial. We take the reciprocal of the temperature parameter to transform it back to the range 0-infinity.\n5. We calculate the likelihood of the data given the model numpyro.sample( \u0026#34;observed_choices\u0026#34;, dist.Bernoulli(probs=choice_p), obs=choices, )\rFinally, we calculate the likelihood of the observed choices given the model. We use a Bernoulli distribution to model the likelihood of the observed choices, with the probability of each choice being given by the softmax function applied to the expected values. The Bernoulli distribution is a good choice for binary data, as it models the probability of a single binary outcome.\nSampling Now that we have our model set up, we can sample from it using MCMC. We\u0026rsquo;ll use the NUTS sampler, which is a good general-purpose sampler that works well for many models.\nThere are few important settings for the sampler that it\u0026rsquo;s worth being aware of:\nnum_warmup - the number of warmup steps to take. This is the number of steps the sampler takes to \u0026ldquo;warm up\u0026rdquo; before it starts sampling. During warmup, the sampler adapts its step size and other parameters to try to find a good region of the parameter space to sample from. num_samples - the number of samples to take after warmup. This is the number of samples that the sampler will take after warmup to estimate the posterior distribution. num_chains - the number of chains to run. Running multiple chains can help to diagnose problems with the sampler, as you can compare the results from different chains to see if they agree. Here, we\u0026rsquo;ll set the number of warmups and samples to a lower number for the sake of speed, but in practice you would want to run more warmup and samples to get a better estimate of the posterior distribution.\nfrom numpyro.infer import MCMC, NUTS # Sampling settings N_SAMPLES = 4000 # This should be higher in practice N_WARMUP = 2000 # This should be higher in practice N_CHAINS = 4 # Set up the NUTS sampler for our model nuts_kernel = NUTS(asymmetric_rescorla_wagner_statistical_model) # Set up the MCMC object mcmc = MCMC( nuts_kernel, num_samples=N_SAMPLES, num_warmup=N_WARMUP, num_chains=N_CHAINS ) # Set the random key for sampling rng_key = jax.random.PRNGKey(0) # Run the sampler mcmc.run(rng_key, rewards, choices) # Get the samples samples = mcmc.get_samples()\r0%| | 0/6000 [00:00\u0026lt;?, ?it/s] 0%| | 0/6000 [00:00\u0026lt;?, ?it/s] 0%| | 0/6000 [00:00\u0026lt;?, ?it/s] 0%| | 0/6000 [00:00\u0026lt;?, ?it/s] Diagnostics It\u0026rsquo;s worth checking that the sampling procedure has gone as it should do. There are various ways to do this, many of which are implemented in Arviz, and we won\u0026rsquo;t cover them in detail here. One quick check we can run is to look at the traceplot of the samples, which shows the value of each parameter over the course of the sampling procedure. This can give us a sense of whether the sampler has converged to the posterior distribution.\n# TO BE IMPLEMENTED\rParameter recovery We can also check how well our model recovers true parameter values. Because we have fit it to simulated data here, we can compare the estimated parameter values to the true parameter values we used to generate the data. This can give us a sense of how well our model is able to recover the true parameters.\nSome useful functions for doing this are included in our model_fit_tools package. For example, we can easily plot the estimated parameter values against the true parameter values to see how well they match up using the plot_recovery function.\nfrom model_fit_tools.plotting import plot_recovery # Plot the recovery of thhe parameters plot_recovery( np.stack([alpha_p, alpha_n, temperature]).T, np.dstack([samples[\u0026#39;alpha_p_subject_param\u0026#39;], samples[\u0026#39;alpha_n_subject_param\u0026#39;], samples[\u0026#39;temperature_subject_param\u0026#39;]]), [\u0026#34;alpha_p\u0026#34;, \u0026#34;alpha_n\u0026#34;, \u0026#34;temperature\u0026#34;], )\rAs you can see, the values are correlated but they\u0026rsquo;re not perfect (in particular, the range of values for alpha_ is quite constrained). This is likely due to the fact that the \u0026ldquo;task\u0026rdquo; here is not really designed to be a good test of the model.\n","date":"0001-01-01","id":19,"permalink":"/docs/computational_modelling/tutorial/6.-model-fitting-using-mcmc/","summary":"Model fitting using MCMC Next, we\u0026rsquo;ll try fitting our model to some simulated data.\nImports First, we import necessary packages.","tags":[],"title":"6. Model fitting using MCMC"},{"content":"","date":"0001-01-01","id":20,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":21,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"Building models If possible, build models using JAX Make sure code is modular and well-documented Components that could be reused should be made into functions Contribute model code to behavioural_modelling if appropriate Fitting models See this paper for some useful advice on fitting models ","date":"0001-01-01","id":22,"permalink":"/docs/computational_modelling/general-best-practices/","summary":"Building models If possible, build models using JAX Make sure code is modular and well-documented Components that could be reused should be made into functions Contribute model code to behavioural_modelling if appropriate Fitting models See this paper for some useful advice on fitting models ","tags":[],"title":"General best practices"},{"content":"","date":"0001-01-01","id":23,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]